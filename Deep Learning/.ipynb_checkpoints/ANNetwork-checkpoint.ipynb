{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 多分类问题\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 二分类问题\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 均方误差回归问题\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')\n",
    "\n",
    "# 自定义评估标准函数\n",
    "import keras.backend as K\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', mean_pred])\n",
    "\n",
    "\n",
    "# 对于具有 2 个类的单输入模型（二进制分类）：\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 生成虚拟数据\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# 训练模型，以 32 个样本为一个 batch 进行迭代\n",
    "model.fit(data, labels, epochs=10, batch_size=32)\n",
    "\n",
    "# 对于具有 10 个类的单输入模型（多分类分类）：\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 生成虚拟数据\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "# 将标签转换为分类的 one-hot 编码\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "# 训练模型，以 32 个样本为一个 batch 进行迭代\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基于多层感知器 (MLP) 的 softmax 多分类\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# 生成虚拟数据\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) 是一个具有 64 个隐藏神经元的全连接层。\n",
    "# 在第一层必须指定所期望的输入数据尺寸：\n",
    "# 在这里，是一个 20 维的向量。\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "100/100 [==============================] - 0s 997us/step - loss: 0.6917 - accuracy: 0.5500\n",
      "Epoch 2/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6841 - accuracy: 0.5500\n",
      "Epoch 3/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.6790 - accuracy: 0.5800\n",
      "Epoch 4/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6748 - accuracy: 0.5800\n",
      "Epoch 5/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6712 - accuracy: 0.6200\n",
      "Epoch 6/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6679 - accuracy: 0.6000\n",
      "Epoch 7/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.6648 - accuracy: 0.6200\n",
      "Epoch 8/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.6619 - accuracy: 0.6300\n",
      "Epoch 9/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.6592 - accuracy: 0.6500\n",
      "Epoch 10/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6565 - accuracy: 0.6500\n",
      "Epoch 11/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6538 - accuracy: 0.6500\n",
      "Epoch 12/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.6514 - accuracy: 0.6400\n",
      "Epoch 13/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6490 - accuracy: 0.6500\n",
      "Epoch 14/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.6466 - accuracy: 0.6500\n",
      "Epoch 15/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.6443 - accuracy: 0.6600\n",
      "Epoch 16/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.6422 - accuracy: 0.6600\n",
      "Epoch 17/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6401 - accuracy: 0.6700\n",
      "Epoch 18/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.6381 - accuracy: 0.6800\n",
      "Epoch 19/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6362 - accuracy: 0.6900\n",
      "Epoch 20/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6344 - accuracy: 0.6900\n",
      "Epoch 21/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6325 - accuracy: 0.6900\n",
      "Epoch 22/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6306 - accuracy: 0.7000\n",
      "Epoch 23/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.6288 - accuracy: 0.7000\n",
      "Epoch 24/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.6270 - accuracy: 0.7100\n",
      "Epoch 25/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6252 - accuracy: 0.7000\n",
      "Epoch 26/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.6235 - accuracy: 0.7300\n",
      "Epoch 27/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6217 - accuracy: 0.7000\n",
      "Epoch 28/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6199 - accuracy: 0.7400\n",
      "Epoch 29/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.6182 - accuracy: 0.7200\n",
      "Epoch 30/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.6165 - accuracy: 0.7300\n",
      "Epoch 31/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.6148 - accuracy: 0.7200\n",
      "Epoch 32/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.6131 - accuracy: 0.7100\n",
      "Epoch 33/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.6115 - accuracy: 0.7100\n",
      "Epoch 34/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6098 - accuracy: 0.7300\n",
      "Epoch 35/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6082 - accuracy: 0.7100\n",
      "Epoch 36/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6066 - accuracy: 0.7300\n",
      "Epoch 37/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.6050 - accuracy: 0.7100\n",
      "Epoch 38/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.6034 - accuracy: 0.7400\n",
      "Epoch 39/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.6018 - accuracy: 0.7200\n",
      "Epoch 40/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.6002 - accuracy: 0.7400\n",
      "Epoch 41/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.5987 - accuracy: 0.7200\n",
      "Epoch 42/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5971 - accuracy: 0.7400\n",
      "Epoch 43/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5956 - accuracy: 0.7200\n",
      "Epoch 44/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5940 - accuracy: 0.7400\n",
      "Epoch 45/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5925 - accuracy: 0.7300\n",
      "Epoch 46/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5910 - accuracy: 0.7500\n",
      "Epoch 47/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5895 - accuracy: 0.7300\n",
      "Epoch 48/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5880 - accuracy: 0.7500\n",
      "Epoch 49/200\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.5865 - accuracy: 0.7400\n",
      "Epoch 50/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5851 - accuracy: 0.7600\n",
      "Epoch 51/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5836 - accuracy: 0.7400\n",
      "Epoch 52/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.5821 - accuracy: 0.7500\n",
      "Epoch 53/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5806 - accuracy: 0.7400\n",
      "Epoch 54/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5792 - accuracy: 0.7600\n",
      "Epoch 55/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5778 - accuracy: 0.7400\n",
      "Epoch 56/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5764 - accuracy: 0.7600\n",
      "Epoch 57/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5750 - accuracy: 0.7400\n",
      "Epoch 58/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.5737 - accuracy: 0.7600\n",
      "Epoch 59/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5723 - accuracy: 0.7400\n",
      "Epoch 60/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.5709 - accuracy: 0.7600\n",
      "Epoch 61/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5695 - accuracy: 0.7400\n",
      "Epoch 62/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5681 - accuracy: 0.7600\n",
      "Epoch 63/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5668 - accuracy: 0.7400\n",
      "Epoch 64/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5654 - accuracy: 0.7700\n",
      "Epoch 65/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5641 - accuracy: 0.7400\n",
      "Epoch 66/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5627 - accuracy: 0.7600\n",
      "Epoch 67/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5614 - accuracy: 0.7400\n",
      "Epoch 68/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5601 - accuracy: 0.7600\n",
      "Epoch 69/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5587 - accuracy: 0.7400\n",
      "Epoch 70/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5574 - accuracy: 0.7600\n",
      "Epoch 71/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5561 - accuracy: 0.7500\n",
      "Epoch 72/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.5549 - accuracy: 0.7600\n",
      "Epoch 73/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5536 - accuracy: 0.7500\n",
      "Epoch 74/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5524 - accuracy: 0.7600\n",
      "Epoch 75/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5511 - accuracy: 0.7500\n",
      "Epoch 76/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5499 - accuracy: 0.7600\n",
      "Epoch 77/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5486 - accuracy: 0.7500\n",
      "Epoch 78/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5474 - accuracy: 0.7700\n",
      "Epoch 79/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5462 - accuracy: 0.7400\n",
      "Epoch 80/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5451 - accuracy: 0.7700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5438 - accuracy: 0.7500\n",
      "Epoch 82/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5426 - accuracy: 0.7700\n",
      "Epoch 83/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5414 - accuracy: 0.7500\n",
      "Epoch 84/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.5402 - accuracy: 0.7800\n",
      "Epoch 85/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5390 - accuracy: 0.7500\n",
      "Epoch 86/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5378 - accuracy: 0.7800\n",
      "Epoch 87/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5367 - accuracy: 0.7500\n",
      "Epoch 88/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5356 - accuracy: 0.7900\n",
      "Epoch 89/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5344 - accuracy: 0.7500\n",
      "Epoch 90/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5333 - accuracy: 0.7900\n",
      "Epoch 91/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5321 - accuracy: 0.7500\n",
      "Epoch 92/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5310 - accuracy: 0.7900\n",
      "Epoch 93/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5298 - accuracy: 0.7500\n",
      "Epoch 94/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5287 - accuracy: 0.7900\n",
      "Epoch 95/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5276 - accuracy: 0.7500\n",
      "Epoch 96/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5265 - accuracy: 0.8000\n",
      "Epoch 97/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5255 - accuracy: 0.7500\n",
      "Epoch 98/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5243 - accuracy: 0.8100\n",
      "Epoch 99/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5232 - accuracy: 0.7600\n",
      "Epoch 100/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5221 - accuracy: 0.8100\n",
      "Epoch 101/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.5211 - accuracy: 0.7800\n",
      "Epoch 102/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5201 - accuracy: 0.8200\n",
      "Epoch 103/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5190 - accuracy: 0.7800\n",
      "Epoch 104/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5179 - accuracy: 0.8200\n",
      "Epoch 105/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5167 - accuracy: 0.7800\n",
      "Epoch 106/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5156 - accuracy: 0.8200\n",
      "Epoch 107/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5145 - accuracy: 0.7800\n",
      "Epoch 108/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5134 - accuracy: 0.8200\n",
      "Epoch 109/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5123 - accuracy: 0.7800\n",
      "Epoch 110/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5113 - accuracy: 0.8200\n",
      "Epoch 111/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5103 - accuracy: 0.7900\n",
      "Epoch 112/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5094 - accuracy: 0.8200\n",
      "Epoch 113/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5083 - accuracy: 0.7900\n",
      "Epoch 114/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5073 - accuracy: 0.8200\n",
      "Epoch 115/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5064 - accuracy: 0.8000\n",
      "Epoch 116/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.5052 - accuracy: 0.8200\n",
      "Epoch 117/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.5041 - accuracy: 0.8000\n",
      "Epoch 118/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.5029 - accuracy: 0.8200\n",
      "Epoch 119/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.5019 - accuracy: 0.8000\n",
      "Epoch 120/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.5009 - accuracy: 0.8200\n",
      "Epoch 121/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4999 - accuracy: 0.8000\n",
      "Epoch 122/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.4987 - accuracy: 0.8200\n",
      "Epoch 123/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4978 - accuracy: 0.8000\n",
      "Epoch 124/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4967 - accuracy: 0.8200\n",
      "Epoch 125/200\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.4957 - accuracy: 0.8000\n",
      "Epoch 126/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4948 - accuracy: 0.8200\n",
      "Epoch 127/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.4937 - accuracy: 0.8000\n",
      "Epoch 128/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4926 - accuracy: 0.8200\n",
      "Epoch 129/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.4917 - accuracy: 0.8100\n",
      "Epoch 130/200\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.4906 - accuracy: 0.8200\n",
      "Epoch 131/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.4896 - accuracy: 0.8100\n",
      "Epoch 132/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.4886 - accuracy: 0.8200\n",
      "Epoch 133/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4877 - accuracy: 0.8100\n",
      "Epoch 134/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4867 - accuracy: 0.8200\n",
      "Epoch 135/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.4857 - accuracy: 0.8100\n",
      "Epoch 136/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4847 - accuracy: 0.8300\n",
      "Epoch 137/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.4837 - accuracy: 0.8200\n",
      "Epoch 138/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4828 - accuracy: 0.8300\n",
      "Epoch 139/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4817 - accuracy: 0.8100\n",
      "Epoch 140/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4808 - accuracy: 0.8300\n",
      "Epoch 141/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4797 - accuracy: 0.8200\n",
      "Epoch 142/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4787 - accuracy: 0.8300\n",
      "Epoch 143/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.4777 - accuracy: 0.8200\n",
      "Epoch 144/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4768 - accuracy: 0.8500\n",
      "Epoch 145/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4758 - accuracy: 0.8200\n",
      "Epoch 146/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4748 - accuracy: 0.8500\n",
      "Epoch 147/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4739 - accuracy: 0.8200\n",
      "Epoch 148/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.4730 - accuracy: 0.8500\n",
      "Epoch 149/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.4720 - accuracy: 0.8200\n",
      "Epoch 150/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.4711 - accuracy: 0.8600\n",
      "Epoch 151/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4701 - accuracy: 0.8200\n",
      "Epoch 152/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4692 - accuracy: 0.8600\n",
      "Epoch 153/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.4681 - accuracy: 0.8200\n",
      "Epoch 154/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4671 - accuracy: 0.8500\n",
      "Epoch 155/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4662 - accuracy: 0.8200\n",
      "Epoch 156/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4653 - accuracy: 0.8600\n",
      "Epoch 157/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.4643 - accuracy: 0.8200\n",
      "Epoch 158/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4635 - accuracy: 0.8600\n",
      "Epoch 159/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4625 - accuracy: 0.8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4616 - accuracy: 0.8600\n",
      "Epoch 161/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4607 - accuracy: 0.8200\n",
      "Epoch 162/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4598 - accuracy: 0.8600\n",
      "Epoch 163/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4587 - accuracy: 0.8200\n",
      "Epoch 164/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.4578 - accuracy: 0.8600\n",
      "Epoch 165/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4569 - accuracy: 0.8200\n",
      "Epoch 166/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.4558 - accuracy: 0.8600\n",
      "Epoch 167/200\n",
      "100/100 [==============================] - 0s 40us/step - loss: 0.4549 - accuracy: 0.8200\n",
      "Epoch 168/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4540 - accuracy: 0.8600\n",
      "Epoch 169/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.4531 - accuracy: 0.8200\n",
      "Epoch 170/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4524 - accuracy: 0.8600\n",
      "Epoch 171/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4515 - accuracy: 0.8200\n",
      "Epoch 172/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.4507 - accuracy: 0.8700\n",
      "Epoch 173/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4497 - accuracy: 0.8200\n",
      "Epoch 174/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4487 - accuracy: 0.8700\n",
      "Epoch 175/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4478 - accuracy: 0.8200\n",
      "Epoch 176/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4469 - accuracy: 0.8600\n",
      "Epoch 177/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.4460 - accuracy: 0.8300\n",
      "Epoch 178/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.4451 - accuracy: 0.8600\n",
      "Epoch 179/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4442 - accuracy: 0.8300\n",
      "Epoch 180/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.4433 - accuracy: 0.8600\n",
      "Epoch 181/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4426 - accuracy: 0.8300\n",
      "Epoch 182/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4416 - accuracy: 0.8700\n",
      "Epoch 183/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4407 - accuracy: 0.8300\n",
      "Epoch 184/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4399 - accuracy: 0.8700\n",
      "Epoch 185/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4389 - accuracy: 0.8300\n",
      "Epoch 186/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4381 - accuracy: 0.8700\n",
      "Epoch 187/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4372 - accuracy: 0.8300\n",
      "Epoch 188/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4364 - accuracy: 0.8700\n",
      "Epoch 189/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4355 - accuracy: 0.8300\n",
      "Epoch 190/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4346 - accuracy: 0.8700\n",
      "Epoch 191/200\n",
      "100/100 [==============================] - 0s 10us/step - loss: 0.4336 - accuracy: 0.8300\n",
      "Epoch 192/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.4328 - accuracy: 0.8700\n",
      "Epoch 193/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4320 - accuracy: 0.8300\n",
      "Epoch 194/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4312 - accuracy: 0.8700\n",
      "Epoch 195/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.4303 - accuracy: 0.8300\n",
      "Epoch 196/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4295 - accuracy: 0.8800\n",
      "Epoch 197/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4286 - accuracy: 0.8300\n",
      "Epoch 198/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4278 - accuracy: 0.8800\n",
      "Epoch 199/200\n",
      "100/100 [==============================] - 0s 20us/step - loss: 0.4268 - accuracy: 0.8300\n",
      "Epoch 200/200\n",
      "100/100 [==============================] - 0s 30us/step - loss: 0.4259 - accuracy: 0.8800\n",
      "10/10 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "#基于多层感知器的二分类：\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# 生成虚拟数据\n",
    "x_train = np.random.random((100, 20))\n",
    "y_train = np.random.randint(2, size=(100, 1))\n",
    "x_test = np.random.random((10, 20))\n",
    "y_test = np.random.randint(2, size=(10, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(1, input_dim=20, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=200,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential， load_model\n",
    "#from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation, LeakyReLU\n",
    "#from keras.layers.noise import AlphaDropout\n",
    "#from keras.utils.generic_utils import get_custom_objects\n",
    "#from keras import backend as K\n",
    "#from keras.optimizers import Adam\n",
    "#from keras.models import load_model\n",
    "\n",
    "# LOAD DATA\n",
    "train_data = np.load('add_02.npz')\n",
    "x_train = train_data['x']\n",
    "y_train = train_data['y']\n",
    "\n",
    "#print(x_train)\n",
    "#print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readying neural network model\n",
    "def build_cnn(activation, dropout_rate, optimizer):\n",
    "    model = Sequential()\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, input_shape=(3,))) \n",
    "    model.add(Activation(activation))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', #损失函数（loss function），模型要将其最小化，可以通过字符串标识符/目标函数指定\n",
    "        optimizer=optimizer, #优化器（opyimizer）,如rmsprop、adagrad，或一个Optimizer类的对象\n",
    "        metrics=['accuracy'] #指标（metricts）列表， 对于任何分类问题，需要将其设置为metrics = [‘accuracy’]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Add the GELU function to Keras\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "get_custom_objects().update({'gelu': Activation(gelu)})\n",
    "\n",
    "#act_func = ['sigmoid', 'relu', 'elu', 'leaky-relu', 'selu', 'gelu']\n",
    "act_func = ['sigmoid']#扩展型指数线性单元激活函数（SELU）高斯误差线性单元激活函数（GELU）\n",
    "\n",
    "\n",
    "#model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "\n",
    "# identical to the previous one\n",
    "#model = load_model('my_model.h5')\n",
    "\n",
    "result = []\n",
    "\n",
    "for activation in act_func:\n",
    "    print('\\nTraining with -->{0}<-- activation function\\n'.format(activation))\n",
    "    \n",
    "    model = build_cnn(activation=activation,\n",
    "                      dropout_rate=0.2,\n",
    "                      optimizer=Adam(clipvalue=0.5))\n",
    "    '''\n",
    "    model.fit(x=None, y=None, \n",
    "        batch_size=None, \n",
    "        epochs=1, \n",
    "        verbose=1, \n",
    "        callbacks=None, \n",
    "        validation_split=0.0, validation_data=None, \n",
    "        shuffle=True, \n",
    "        class_weight=None, sample_weight=None, \n",
    "        initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1)\n",
    "    '''\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        batch_size=None, # 128 is faster, but less accurate. 16/32 recommended\n",
    "                        epochs=100,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.0,\n",
    "                        validation_data=None ) \n",
    "    \n",
    "    result.append(history)\n",
    "    \n",
    "    K.clear_session()\n",
    "    del model\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 (4 neurons, each with 3 inputs): w_ij \n",
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
      " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
      "Layer 2 (1 neuron, with 4 inputs): w_jk \n",
      "[[-0.5910955 ]\n",
      " [ 0.75623487]\n",
      " [-0.94522481]\n",
      " [ 0.34093502]]\n",
      "loss:  3.4499052034902866\n",
      "loss:  0.05773908026082919\n",
      "loss:  0.018671717892659118\n",
      "loss:  0.011126011881028647\n",
      "loss:  0.007921059663965674\n",
      "loss:  0.006148732897449158\n",
      "loss:  0.005024147519876531\n",
      "loss:  0.004247151266496663\n",
      "loss:  0.0036782010788174975\n",
      "loss:  0.0032436221726850623\n",
      "New synaptic weights after training: \n",
      "w_ij:\n",
      "  [[  0.15412064   5.61526375  -6.92492097 -10.55136518]\n",
      " [  0.13145897 -10.51956567  -6.93646143   5.44436653]\n",
      " [ -0.01611526  -0.75294181   0.19272139  -0.54865495]]\n",
      "w_jk:\n",
      " [[-13.08177073]\n",
      " [ 15.15669436]\n",
      " [-31.14768507]\n",
      " [ 15.0867679 ]]\n",
      "Considering new situation [1, 0, 0] -> ?: \n",
      "[[0.99967377]]\n"
     ]
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "import numpy as np\n",
    "from numpy import exp, array, random, dot, ones_like, where, log10\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create our Artificial Neural Network class\n",
    "class ArtificialNeuralNetwork():\n",
    "    \n",
    "    # initializing the class\n",
    "    def __init__(self):\n",
    "        \n",
    "        # generating the same synaptic weights every time the program runs\n",
    "        random.seed(1)\n",
    "        \n",
    "        # synaptic weights (3 × 4 Matrix) of the hidden layer \n",
    "        self.w_ij = 2 * random.rand(3, 4) - 1\n",
    "        \n",
    "        # synaptic weights (4 × 1 Matrix) of the output layer\n",
    "        self.w_jk = 2 * random.rand(4, 1) - 1\n",
    "       \n",
    "        \n",
    "    def Sigmoid(self, x):\n",
    "        \n",
    "        # The Sigmoid activation function will turn every input value into probabilities between 0 and 1\n",
    "        # the probabilistic values help us assert which class x belongs to\n",
    "        \n",
    "        return 1 / (1 + exp(-x))\n",
    "        \n",
    "        #return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "    \n",
    "    def SigmoidDerivative(self, x):\n",
    "        \n",
    "        # The derivative of Sigmoid will be used to calculate the gradient during the backpropagation process\n",
    "        # and help optimize the random starting synaptic weights\n",
    "        \n",
    "        return x * (1 - x)\n",
    "        \n",
    "        #ax = (0.0356774 * tf.pow(x, 3) + 0.797885 * x)\n",
    "        #xx = (0.5 * tf.tanh(ax) + (0.0535161 * tf.pow(x, 3) + 0.398942 * x) * tf.pow(tf.sech(ax), 2) + 0.5)\n",
    "              \n",
    "        #return xx\n",
    "        \n",
    "    def crossentropyerror(self, a, y):\n",
    "        \n",
    "        # The cross entropy loss function\n",
    "        # we use it to evaluate the performance of our model\n",
    "        \n",
    "        return - sum(y * log10(a) + (1 - y) * log10(1 - a))\n",
    "    \n",
    "    def mael1(self, a, y):\n",
    "        \n",
    "        return np.sum(np.absolute(a - y))\n",
    "    \n",
    "    def train(self, x, y, learning_rate, iterations):\n",
    "        \n",
    "        # x: training set of data\n",
    "        # y: the actual output of the training data\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            z_ij = dot(x, self.w_ij) # the dot product of the weights of the hidden layer and the inputs\n",
    "            a_ij = self.Sigmoid(z_ij) # applying the Sigmoid activation function\n",
    "            \n",
    "            z_jk = dot(a_ij, self.w_jk) # the same previous process will be applied to find the predicted output\n",
    "            a_jk = self.Sigmoid(z_jk)  \n",
    "            \n",
    "            dl_jk = -y/a_jk + (1 - y)/(1 - a_jk) # the derivative of the cross entropy loss wrt output\n",
    "            da_jk = self.SigmoidDerivative(a_jk) # the derivative of Sigmoid  wrt the input (before activ.) of the output layer\n",
    "            dz_jk = a_ij # the derivative of the inputs of the hidden layer (before activation) wrt weights of the output layer\n",
    "            \n",
    "            dl_ij = dot(da_jk * dl_jk, self.w_jk.T) # the derivative of cross entropy loss wrt hidden layer input (after activ.)\n",
    "            da_ij = self.SigmoidDerivative(a_ij) # the derivative of Sigmoid wrt the inputs of the hidden layer (before activ.)\n",
    "            dz_ij = x # the derivative of the inputs of the hidden layer (before activation) wrt weights of the hidden layer\n",
    "            \n",
    "            # calculating the gradient using the chain rule\n",
    "            gradient_ij = dot(dz_ij.T , dl_ij * da_ij)\n",
    "            gradient_jk = dot(dz_jk.T , dl_jk * da_jk)\n",
    "            \n",
    "            # calculating the new optimal weights\n",
    "            self.w_ij = self.w_ij - learning_rate * gradient_ij \n",
    "            self.w_jk = self.w_jk - learning_rate * gradient_jk\n",
    "            \n",
    "            # printing the loss of our neural network after each 1000 iteration\n",
    "            if i % 1000 == 0 in range(iterations):\n",
    "                #print(\"loss: \", self.crossentropyerror(a_jk, y))\n",
    "                print(\"loss: \", self.mael1(a_jk, y))\n",
    "                  \n",
    "    def predict(self, inputs):\n",
    "        \n",
    "        # predicting the class of the input data after weights optimization\n",
    "        \n",
    "        output_from_layer1 = self.Sigmoid(dot(inputs, self.w_ij)) # the output of the hidden layer\n",
    "        \n",
    "        output_from_layer2 = self.Sigmoid(dot(output_from_layer1, self.w_jk)) # the output of the output layer\n",
    "        \n",
    "        return output_from_layer2\n",
    "    \n",
    "    # the function will print the initial starting weights before training\n",
    "    def SynapticWeights(self):\n",
    "        \n",
    "        print(\"Layer 1 (4 neurons, each with 3 inputs): w_ij \")        \n",
    "        print(self.w_ij)\n",
    "        \n",
    "        print(\"Layer 2 (1 neuron, with 4 inputs): w_jk \")        \n",
    "        print(self.w_jk)\n",
    "\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    ANN = ArtificialNeuralNetwork()\n",
    "    \n",
    "    ANN.SynapticWeights()\n",
    "    \n",
    "    # the training inputs \n",
    "    # the last column is used to add non linearity to the clasification task\n",
    "    x = array([[0, 0, 1], \n",
    "               [0, 1, 1], \n",
    "               [1, 0, 1], \n",
    "               [0, 1, 0], \n",
    "               [1, 0, 0], \n",
    "               [1, 1, 1], \n",
    "               [0, 0, 0]])\n",
    "    \n",
    "    # the training outputs\n",
    "    y = array([[0, 1, 1, 1, 1, 0, 0]]).T\n",
    "    #y = array([[3, 5, 5, 5, 5, 3, 3]]).T\n",
    "\n",
    "    ANN.train(x, y, 1, 10000)\n",
    "    \n",
    "    # Printing the new synaptic weights after training\n",
    "    print(\"New synaptic weights after training: \")\n",
    "    print(\"w_ij:\\n \", ANN.w_ij)\n",
    "    print(\"w_jk:\\n\", ANN.w_jk)\n",
    "    \n",
    "    # Our prediction after feeding the ANN with new set of data\n",
    "    print(\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "    print(ANN.predict(array([[1, 0, 0]])))\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
