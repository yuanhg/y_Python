{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此练习是Sigmoid全过程演示神经网络如何工作，同时也可以使用如Keras等库，屏蔽神经网络内部过程\n",
    "# Import our dependencies\n",
    "import numpy as np\n",
    "from numpy import exp, array, random, dot, ones_like, where, log10\n",
    "\n",
    "# Create our Artificial Neural Network class\n",
    "class ANN_Sigmoid():\n",
    "    \n",
    "    # initializing the class\n",
    "    def __init__(self):\n",
    "        \n",
    "        # generating the same synaptic weights every time the program runs\n",
    "        random.seed(1)\n",
    "        \n",
    "        # synaptic weights (3 × 4 Matrix) of the hidden layer \n",
    "        self.w_ij = 2 * random.rand(3, 4) - 1\n",
    "        \n",
    "        # synaptic weights (4 × 1 Matrix) of the output layer\n",
    "        self.w_jk = 2 * random.rand(4, 1) - 1\n",
    "        \n",
    "    def Sigmoid(self, x):\n",
    "        \n",
    "        # The Sigmoid activation function will turn every input value into \n",
    "        # probabilities between 0 and 1\n",
    "        # the probabilistic values help us assert which class x belongs to        \n",
    "        return 1 / (1 + exp(-x))\n",
    "    \n",
    "    def SigmoidDerivative(self, x):\n",
    "        \n",
    "        # The derivative of Sigmoid will be used to calculate the gradient during \n",
    "        # the backpropagation process\n",
    "        # and help optimize the random starting synaptic weights        \n",
    "        return x * (1 - x)\n",
    "        \n",
    "    def crossentropyerror(self, a, y):\n",
    "        \n",
    "        # The cross entropy loss function\n",
    "        # we use it to evaluate the performance of our model        \n",
    "        return - sum(y * log10(a) + (1 - y) * log10(1 - a))\n",
    "    \n",
    "    def train(self, x, y, learning_rate, iterations):\n",
    "        \n",
    "        # x: training set of data\n",
    "        # y: the actual output of the training data\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            z_ij = dot(x, self.w_ij) # the dot product of the weights of the hidden layer and the inputs\n",
    "            a_ij = self.Sigmoid(z_ij) # applying the Sigmoid activation function\n",
    "            \n",
    "            z_jk = dot(a_ij, self.w_jk) # the same previous process will be applied to find the predicted output\n",
    "            a_jk = self.Sigmoid(z_jk)  \n",
    "            \n",
    "            dl_jk = -y/a_jk + (1 - y)/(1 - a_jk) # the derivative of the cross entropy loss wrt output\n",
    "            da_jk = self.SigmoidDerivative(a_jk) # the derivative of Sigmoid  wrt the input (before activ.) of the output layer\n",
    "            dz_jk = a_ij # the derivative of the inputs of the hidden layer (before activation) wrt weights of the output layer\n",
    "            \n",
    "            dl_ij = dot(da_jk * dl_jk, self.w_jk.T) # the derivative of cross entropy loss wrt hidden layer input (after activ.)\n",
    "            da_ij = self.SigmoidDerivative(a_ij) # the derivative of Sigmoid wrt the inputs of the hidden layer (before activ.)\n",
    "            dz_ij = x # the derivative of the inputs of the hidden layer (before activation) wrt weights of the hidden layer\n",
    "            \n",
    "            # calculating the gradient using the chain rule\n",
    "            gradient_ij = dot(dz_ij.T , dl_ij * da_ij)\n",
    "            gradient_jk = dot(dz_jk.T , dl_jk * da_jk)\n",
    "            \n",
    "            # calculating the new optimal weights\n",
    "            self.w_ij = self.w_ij - learning_rate * gradient_ij \n",
    "            self.w_jk = self.w_jk - learning_rate * gradient_jk\n",
    "            \n",
    "            # printing the loss of our neural network after each 1000 iteration\n",
    "            if i % 1000 == 0 in range(iterations):\n",
    "                print(\"loss: \", self.crossentropyerror(a_jk, y))\n",
    "                #print(\"loss: \", self.mael1(a_jk, y))\n",
    "                  \n",
    "    def predict(self, inputs):\n",
    "        \n",
    "        # predicting the class of the input data after weights optimization\n",
    "        # the output of the hidden layer\n",
    "        output_from_layer1 = self.Sigmoid(dot(inputs, self.w_ij)) \n",
    "        # the output of the output layer\n",
    "        output_from_layer2 = self.Sigmoid(dot(output_from_layer1, self.w_jk))   \n",
    "        \n",
    "        return output_from_layer2\n",
    "    \n",
    "    # the function will print the initial starting weights before training\n",
    "    def SynapticWeights(self):\n",
    "        \n",
    "        print(\"Layer 1 : w_ij \")        \n",
    "        print(self.w_ij)\n",
    "        \n",
    "        print(\"Layer 2 : w_jk \")        \n",
    "        print(self.w_jk)   \n",
    "\n",
    "def main():\n",
    "    \n",
    "    ANN = ANN_Sigmoid()    \n",
    "    #ANN.SynapticWeights()\n",
    "    \n",
    "    # the training inputs \n",
    "    # the last column is used to add non linearity to the clasification task\n",
    "\n",
    "    x = array([[0, 0, 1], \n",
    "               [0, 1, 1], \n",
    "               [1, 0, 1], \n",
    "               [0, 1, 0], \n",
    "               [1, 0, 0], \n",
    "               [1, 1, 1], \n",
    "               [0, 0, 0]])\n",
    "    \n",
    "    # the training outputs\n",
    "    y = array([[0, 1, 1, 1, 1, 0, 0]]).T\n",
    "\n",
    "    ANN.train(x, y, 1, 10000)\n",
    "    \n",
    "    # Printing the new synaptic weights after training\n",
    "    #print(\"New synaptic weights after training: \")\n",
    "    #print(\"Layer 1  w_ij:\\n\", ANN.w_ij)\n",
    "    #print(\"Layer 2  w_jk:\\n\", ANN.w_jk)\n",
    "    \n",
    "    # Our prediction after feeding the ANN with new set of data\n",
    "    print(\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "    print(ANN.predict(array([[1, 0, 0]])))\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此练习是Relu全过程演示神经网络如何工作，同时也可以使用如Keras等库，屏蔽神经网络内部过程\n",
    "# Import our dependencies\n",
    "import numpy as np\n",
    "from numpy import exp, array, random, dot, ones_like, where, log10\n",
    "\n",
    "# Create our Artificial Neural Network class\n",
    "class ANN_Relu():\n",
    "    \n",
    "    # initializing the class\n",
    "    def __init__(self):\n",
    "        \n",
    "        # generating the same synaptic weights every time the program runs\n",
    "        random.seed(1)\n",
    "        \n",
    "        # synaptic weights (2 × 256 Matrix) of the hidden layer \n",
    "        self.w_ij = 2 * random.rand(2, 64) - 1\n",
    "        \n",
    "        # synaptic weights (256 × 1 Matrix) of the output layer\n",
    "        self.w_jk = 2 * random.rand(64, 1) - 1\n",
    "        \n",
    "    def Relu(self, x):\n",
    "                      \n",
    "        return np.where(x > 0, x, 0)\n",
    "    \n",
    "    def ReluDerivative(self, x):\n",
    "               \n",
    "        return np.where(x > 0, 1, 0)\n",
    "        \n",
    "    def crossentropyerror(self, a, y):\n",
    "        \n",
    "        # The cross entropy loss function\n",
    "        # we use it to evaluate the performance of our model        \n",
    "        return - sum(y * log10(a) + (1 - y) * log10(1 - a))\n",
    "        #def mael1(self, a, y):        \n",
    "        #return np.sum(a - y)\n",
    "    \n",
    "    def train(self, x, y, learning_rate, iterations):\n",
    "        \n",
    "        # x: training set of data\n",
    "        # y: the actual output of the training data\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            z_ij = dot(x, self.w_ij) # the dot product of the weights of the hidden layer and the inputs\n",
    "            a_ij = self.Relu(z_ij) # applying the Sigmoid activation function\n",
    "            \n",
    "            z_jk = dot(a_ij, self.w_jk) # the same previous process will be applied to find the predicted output\n",
    "            a_jk = self.Relu(z_jk)  \n",
    "            \n",
    "            dl_jk = -y/a_jk + (1 - y)/(1 - a_jk) # the derivative of the cross entropy loss wrt output\n",
    "            da_jk = self.ReluDerivative(a_jk) # the derivative of Sigmoid  wrt the input (before activ.) of the output layer\n",
    "            dz_jk = a_ij # the derivative of the inputs of the hidden layer (before activation) wrt weights of the output layer\n",
    "            \n",
    "            dl_ij = dot(da_jk * dl_jk, self.w_jk.T) # the derivative of cross entropy loss wrt hidden layer input (after activ.)\n",
    "            da_ij = self.ReluDerivative(a_ij) # the derivative of Sigmoid wrt the inputs of the hidden layer (before activ.)\n",
    "            dz_ij = x # the derivative of the inputs of the hidden layer (before activation) wrt weights of the hidden layer\n",
    "            \n",
    "            # calculating the gradient using the chain rule\n",
    "            gradient_ij = dot(dz_ij.T , dl_ij * da_ij)\n",
    "            gradient_jk = dot(dz_jk.T , dl_jk * da_jk)\n",
    "            \n",
    "            # calculating the new optimal weights\n",
    "            self.w_ij = self.w_ij - learning_rate * gradient_ij \n",
    "            self.w_jk = self.w_jk - learning_rate * gradient_jk\n",
    "            \n",
    "            # printing the loss of our neural network after each 1000 iteration\n",
    "            if i % 1000 == 0 in range(iterations):\n",
    "                print(\"loss: \", self.crossentropyerror(a_jk, y))\n",
    "                                  \n",
    "    def predict(self, inputs):\n",
    "        \n",
    "        # predicting the class of the input data after weights optimization\n",
    "        # the output of the hidden layer\n",
    "        output_from_layer1 = self.Relu(dot(inputs, self.w_ij)) \n",
    "        # the output of the output layer\n",
    "        output_from_layer2 = self.Relu(dot(output_from_layer1, self.w_jk))   \n",
    "        \n",
    "        return output_from_layer2\n",
    "    \n",
    "    # the function will print the initial starting weights before training\n",
    "    def SynapticWeights(self):\n",
    "        \n",
    "        print(\"Layer 1 : w_ij \")        \n",
    "        print(self.w_ij)        \n",
    "        print(\"Layer 2 : w_jk \")        \n",
    "        print(self.w_jk)   \n",
    "\n",
    "def main():\n",
    "    \n",
    "    ANN = ANN_Relu()    \n",
    "    #ANN.SynapticWeights()\n",
    "    \n",
    "    # LOAD DATA\n",
    "    train_data = np.load('add_4.npz')\n",
    "    x = train_data['x']\n",
    "    y = train_data['y']\n",
    "\n",
    "    ANN.train(x, y, 0.5, 10000)\n",
    "    \n",
    "    # Printing the new synaptic weights after training\n",
    "    #print(\"New synaptic weights after training: \")\n",
    "    #print(\"Layer 1  w_ij:\\n\", ANN.w_ij)\n",
    "    #print(\"Layer 2  w_jk:\\n\", ANN.w_jk)\n",
    "    \n",
    "    # Our prediction after feeding the ANN with new set of data\n",
    "    print(\"Considering new situation [1, 0] -> ?: \")\n",
    "    print(ANN.predict(array([[1, 0]])))\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此练习是Elu全过程演示神经网络如何工作，同时也可以使用如Keras等库，屏蔽神经网络内部过程\n",
    "# Import our dependencies\n",
    "import numpy as np\n",
    "from numpy import exp, array, random, dot, ones_like, where, log10\n",
    "\n",
    "# Create our Artificial Neural Network class\n",
    "class ANN_Elu():\n",
    "    \n",
    "    # initializing the class\n",
    "    def __init__(self):\n",
    "        \n",
    "        # generating the same synaptic weights every time the program runs\n",
    "        random.seed(1)\n",
    "        \n",
    "        # synaptic weights (2 × 64 Matrix) of the hidden layer \n",
    "        self.w_ij = 2 * random.rand(2, 64) - 1\n",
    "        \n",
    "        # synaptic weights (64 × 1 Matrix) of the output layer\n",
    "        self.w_jk = 2 * random.rand(64, 1) - 1\n",
    "        \n",
    "    def Elu(self, x):\n",
    "                      \n",
    "        return np.where(x > 0, x, 0.1 * (exp(x) - 1))\n",
    "    \n",
    "    def EluDerivative(self, x):\n",
    "               \n",
    "        return np.where(x > 0, 1, 0.1 + 0.1 * (exp(x) - 1))\n",
    "        \n",
    "    def crossentropyerror(self, a, y):\n",
    "        \n",
    "        # The cross entropy loss function\n",
    "        # we use it to evaluate the performance of our model        \n",
    "        return - sum(y * log10(a) + (1 - y) * log10(1 - a))\n",
    "        #def mael1(self, a, y):        \n",
    "        #return np.sum(a - y)\n",
    "    \n",
    "    def train(self, x, y, learning_rate, iterations):\n",
    "        \n",
    "        # x: training set of data\n",
    "        # y: the actual output of the training data\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            z_ij = dot(x, self.w_ij) # the dot product of the weights of the hidden layer and the inputs\n",
    "            a_ij = self.Elu(z_ij) # applying the Sigmoid activation function\n",
    "            \n",
    "            z_jk = dot(a_ij, self.w_jk) # the same previous process will be applied to find the predicted output\n",
    "            a_jk = self.Elu(z_jk)  \n",
    "            \n",
    "            dl_jk = -y/a_jk + (1 - y)/(1 - a_jk) # the derivative of the cross entropy loss wrt output\n",
    "            da_jk = self.EluDerivative(a_jk) # the derivative of Sigmoid  wrt the input (before activ.) of the output layer\n",
    "            dz_jk = a_ij # the derivative of the inputs of the hidden layer (before activation) wrt weights of the output layer\n",
    "            \n",
    "            dl_ij = dot(da_jk * dl_jk, self.w_jk.T) # the derivative of cross entropy loss wrt hidden layer input (after activ.)\n",
    "            da_ij = self.EluDerivative(a_ij) # the derivative of Sigmoid wrt the inputs of the hidden layer (before activ.)\n",
    "            dz_ij = x # the derivative of the inputs of the hidden layer (before activation) wrt weights of the hidden layer\n",
    "            \n",
    "            # calculating the gradient using the chain rule\n",
    "            gradient_ij = dot(dz_ij.T , dl_ij * da_ij)\n",
    "            gradient_jk = dot(dz_jk.T , dl_jk * da_jk)\n",
    "            \n",
    "            # calculating the new optimal weights\n",
    "            self.w_ij = self.w_ij - learning_rate * gradient_ij \n",
    "            self.w_jk = self.w_jk - learning_rate * gradient_jk\n",
    "            \n",
    "            # printing the loss of our neural network after each 1000 iteration\n",
    "            if i % 1000 == 0 in range(iterations):\n",
    "                print(\"loss: \", self.crossentropyerror(a_jk, y))\n",
    "                                  \n",
    "    def predict(self, inputs):\n",
    "        \n",
    "        # predicting the class of the input data after weights optimization\n",
    "        # the output of the hidden layer\n",
    "        output_from_layer1 = self.Elu(dot(inputs, self.w_ij)) \n",
    "        # the output of the output layer\n",
    "        output_from_layer2 = self.Elu(dot(output_from_layer1, self.w_jk))   \n",
    "        \n",
    "        return output_from_layer2\n",
    "    \n",
    "    # the function will print the initial starting weights before training\n",
    "    def SynapticWeights(self):\n",
    "        \n",
    "        print(\"Layer 1 : w_ij \")        \n",
    "        print(self.w_ij)        \n",
    "        print(\"Layer 2 : w_jk \")        \n",
    "        print(self.w_jk)   \n",
    "\n",
    "def main():\n",
    "    \n",
    "    ANN = ANN_Elu()    \n",
    "    #ANN.SynapticWeights()\n",
    "    \n",
    "    # LOAD DATA\n",
    "    train_data = np.load('add_4.npz')\n",
    "    x = train_data['x']\n",
    "    y = train_data['y']\n",
    "\n",
    "    ANN.train(x, y, 0.5, 10000)\n",
    "    \n",
    "    # Printing the new synaptic weights after training\n",
    "    #print(\"New synaptic weights after training: \")\n",
    "    #print(\"Layer 1  w_ij:\\n\", ANN.w_ij)\n",
    "    #print(\"Layer 2  w_jk:\\n\", ANN.w_jk)\n",
    "    \n",
    "    # Our prediction after feeding the ANN with new set of data\n",
    "    print(\"Considering new situation [1, 0] -> ?: \")\n",
    "    print(ANN.predict(array([[1, 0]])))\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此练习是Selu全过程演示神经网络如何工作，同时也可以使用如Keras等库，屏蔽神经网络内部过程\n",
    "# Import our dependencies\n",
    "import numpy as np\n",
    "from numpy import exp, array, random, dot, ones_like, where, log10\n",
    "\n",
    "# Create our Artificial Neural Network class\n",
    "class ANN_Selu():\n",
    "    \n",
    "    # initializing the class\n",
    "    def __init__(self):\n",
    "        \n",
    "        # generating the same synaptic weights every time the program runs\n",
    "        random.seed(1)\n",
    "        \n",
    "        # synaptic weights (2 × 64 Matrix) of the hidden layer \n",
    "        self.w_ij = 2 * random.rand(2, 64) - 1\n",
    "        \n",
    "        # synaptic weights (64 × 1 Matrix) of the output layer\n",
    "        self.w_jk = 2 * random.rand(64, 1) - 1\n",
    "        \n",
    "    def Selu(self, x):\n",
    "                      \n",
    "        return np.where(x > 0, 1.05070098735548 * x, 1.05070098735548 * 1.67326324235438 * (exp(x) -1))\n",
    "    \n",
    "    def SeluDerivative(self, x):\n",
    "               \n",
    "        return np.where(x > 0, 1.05070098735548, 1.05070098735548 * 1.67326324235438 * exp(x))\n",
    "        \n",
    "    def crossentropyerror(self, a, y):\n",
    "        \n",
    "        # The cross entropy loss function\n",
    "        # we use it to evaluate the performance of our model        \n",
    "        return - sum(y * log10(a) + (1 - y) * log10(1 - a))\n",
    "        #def mael1(self, a, y):        \n",
    "        #return np.sum(a - y)\n",
    "    \n",
    "    def train(self, x, y, learning_rate, iterations):\n",
    "        \n",
    "        # x: training set of data\n",
    "        # y: the actual output of the training data\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            z_ij = dot(x, self.w_ij) # the dot product of the weights of the hidden layer and the inputs\n",
    "            a_ij = self.Selu(z_ij) # applying the Sigmoid activation function\n",
    "            \n",
    "            z_jk = dot(a_ij, self.w_jk) # the same previous process will be applied to find the predicted output\n",
    "            a_jk = self.Selu(z_jk)  \n",
    "            \n",
    "            dl_jk = -y/a_jk + (1 - y)/(1 - a_jk) # the derivative of the cross entropy loss wrt output\n",
    "            da_jk = self.SeluDerivative(a_jk) # the derivative of Sigmoid  wrt the input (before activ.) of the output layer\n",
    "            dz_jk = a_ij # the derivative of the inputs of the hidden layer (before activation) wrt weights of the output layer\n",
    "            \n",
    "            dl_ij = dot(da_jk * dl_jk, self.w_jk.T) # the derivative of cross entropy loss wrt hidden layer input (after activ.)\n",
    "            da_ij = self.SeluDerivative(a_ij) # the derivative of Sigmoid wrt the inputs of the hidden layer (before activ.)\n",
    "            dz_ij = x # the derivative of the inputs of the hidden layer (before activation) wrt weights of the hidden layer\n",
    "            \n",
    "            # calculating the gradient using the chain rule\n",
    "            gradient_ij = dot(dz_ij.T , dl_ij * da_ij)\n",
    "            gradient_jk = dot(dz_jk.T , dl_jk * da_jk)\n",
    "            \n",
    "            # calculating the new optimal weights\n",
    "            self.w_ij = self.w_ij - learning_rate * gradient_ij \n",
    "            self.w_jk = self.w_jk - learning_rate * gradient_jk\n",
    "            \n",
    "            # printing the loss of our neural network after each 1000 iteration\n",
    "            if i % 1000 == 0 in range(iterations):\n",
    "                print(\"loss: \", self.crossentropyerror(a_jk, y))\n",
    "                                  \n",
    "    def predict(self, inputs):\n",
    "        \n",
    "        # predicting the class of the input data after weights optimization\n",
    "        # the output of the hidden layer\n",
    "        output_from_layer1 = self.Selu(dot(inputs, self.w_ij)) \n",
    "        # the output of the output layer\n",
    "        output_from_layer2 = self.Selu(dot(output_from_layer1, self.w_jk))   \n",
    "        \n",
    "        return output_from_layer2\n",
    "    \n",
    "    # the function will print the initial starting weights before training\n",
    "    def SynapticWeights(self):\n",
    "        \n",
    "        print(\"Layer 1 : w_ij \")        \n",
    "        print(self.w_ij)        \n",
    "        print(\"Layer 2 : w_jk \")        \n",
    "        print(self.w_jk)   \n",
    "\n",
    "def main():\n",
    "    \n",
    "    ANN = ANN_Selu()    \n",
    "    #ANN.SynapticWeights()\n",
    "    \n",
    "    # LOAD DATA\n",
    "    train_data = np.load('add_4.npz')\n",
    "    x = train_data['x']\n",
    "    y = train_data['y']\n",
    "\n",
    "    ANN.train(x, y, 0.5, 10000)\n",
    "    \n",
    "    # Printing the new synaptic weights after training\n",
    "    #print(\"New synaptic weights after training: \")\n",
    "    #print(\"Layer 1  w_ij:\\n\", ANN.w_ij)\n",
    "    #print(\"Layer 2  w_jk:\\n\", ANN.w_jk)\n",
    "    \n",
    "    # Our prediction after feeding the ANN with new set of data\n",
    "    print(\"Considering new situation [1, 0] -> ?: \")\n",
    "    print(ANN.predict(array([[1, 0]])))\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此练习是Gelu全过程演示神经网络如何工作，同时也可以使用如Keras等库，屏蔽神经网络内部过程\n",
    "# Import our dependencies\n",
    "import numpy as np\n",
    "from numpy import exp, array, random, dot, ones_like, where, log10\n",
    "#import tensorflow as tf\n",
    "\n",
    "# Create our Artificial Neural Network class\n",
    "class ANN_Gelu():\n",
    "    \n",
    "    # initializing the class\n",
    "    def __init__(self):        \n",
    "        # generating the same synaptic weights every time the program runs\n",
    "        random.seed(1)        \n",
    "        # synaptic weights (2 × 64 Matrix) of the hidden layer \n",
    "        self.w_ij = 2 * random.rand(2, 4) - 1        \n",
    "        # synaptic weights (64 × 1 Matrix) of the output layer\n",
    "        self.w_jk = 2 * random.rand(4, 1) - 1\n",
    "\n",
    "    def Gelu(self, x):\n",
    "        #return 0.5*x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "        return 0.5*x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "    \n",
    "    def GeluDerivative(self, x): \n",
    "        \"\"\"\n",
    "        ax = (0.0356774 * tf.pow(x, 3) + 0.797885 * x)\n",
    "        bx = (0.5 * tf.tanh(ax) +\n",
    "              (0.0535161 * tf.pow(x, 3) + 0.398942 * x) * tf.pow(tf.sech(ax), 2) + 0.5)\n",
    "        \"\"\"\n",
    "        ax = (0.0356774 * np.power(x, 3) + 0.797885 * x)\n",
    "        #　sech / 双曲正割： sech(x) = 1 / cosh(x) \n",
    "        bx = 0.5 * np.tanh(ax)\n",
    "        cx = (0.0535161 * np.power(x, 3) + 0.398942 * x) * np.power(1/np.cosh(ax), 2)\n",
    "              \n",
    "        return bx + cx + 0.5\n",
    "        \n",
    "    def crossentropyerror(self, a, y):        \n",
    "        # The cross entropy loss function\n",
    "        # we use it to evaluate the performance of our model        \n",
    "        return - sum(y * log10(a) + (1 - y) * log10(1 - a))\n",
    "    \n",
    "    def mael1(self, a, y):        \n",
    "        return np.sum(np.absolute(a - y))\n",
    "    \n",
    "    def train(self, x, y, learning_rate=1, iterations=10000):        \n",
    "        # x: training set of data\n",
    "        # y: the actual output of the training data        \n",
    "        for i in range(iterations):            \n",
    "            z_ij = dot(x, self.w_ij) # the dot product of the weights of the hidden layer and the inputs\n",
    "            a_ij = self.Gelu(z_ij) # applying the Sigmoid activation function\n",
    "            \n",
    "            z_jk = dot(a_ij, self.w_jk) # the same previous process will be applied to find the predicted output\n",
    "            a_jk = self.Gelu(z_jk)  \n",
    "            \n",
    "            dl_jk = -y/a_jk + (1 - y)/(1 - a_jk) # the derivative of the cross entropy loss wrt output\n",
    "            da_jk = self.GeluDerivative(a_jk) # the derivative of Gelu wrt the input (before activ.) of the output layer\n",
    "            dz_jk = a_ij # the derivative of the inputs of the hidden layer (before activation) wrt weights of the output layer\n",
    "            \n",
    "            dl_ij = dot(da_jk * dl_jk, self.w_jk.T) # the derivative of cross entropy loss wrt hidden layer input (after activ.)\n",
    "            da_ij = self.GeluDerivative(a_ij) # the derivative of Sigmoid wrt the inputs of the hidden layer (before activ.)\n",
    "            dz_ij = x # the derivative of the inputs of the hidden layer (before activation) wrt weights of the hidden layer\n",
    "            \n",
    "            # calculating the gradient using the chain rule\n",
    "            gradient_ij = dot(dz_ij.T , dl_ij * da_ij)\n",
    "            gradient_jk = dot(dz_jk.T , dl_jk * da_jk)\n",
    "            \n",
    "            # calculating the new optimal weights\n",
    "            self.w_ij = self.w_ij - learning_rate * gradient_ij \n",
    "            self.w_jk = self.w_jk - learning_rate * gradient_jk\n",
    "            \n",
    "            # printing the loss of our neural network after each 1000 iteration\n",
    "            if i % 1000 == 0 in range(iterations):\n",
    "                print(\"loss: \", self.crossentropyerror(a_jk, y))\n",
    "                #print(\"loss: \", self.mael1(a_jk, y))\n",
    "                  \n",
    "    def predict(self, inputs):\n",
    "        \n",
    "        # predicting the class of the input data after weights optimization\n",
    "        # the output of the hidden layer\n",
    "        output_from_layer1 = self.Gelu(dot(inputs, self.w_ij)) \n",
    "        # the output of the output layer\n",
    "        output_from_layer2 = self.Gelu(dot(output_from_layer1, self.w_jk))   \n",
    "        \n",
    "        return output_from_layer2\n",
    "    \n",
    "    # the function will print the initial starting weights before training\n",
    "    def SynapticWeights(self):\n",
    "        \n",
    "        print(\"Layer 1 : w_ij \")        \n",
    "        print(self.w_ij)\n",
    "        \n",
    "        print(\"Layer 2 : w_jk \")        \n",
    "        print(self.w_jk)   \n",
    "\n",
    "def main():\n",
    "    \n",
    "    ANN = ANN_Gelu()    \n",
    "    #ANN.SynapticWeights()\n",
    "    \n",
    "    # the training inputs \n",
    "    # LOAD DATA\n",
    "    train_data = np.load('add_2.npz')\n",
    "    x = train_data['x']\n",
    "    y = train_data['y']\n",
    "    \n",
    "    #print(x)\n",
    "    #print(y)\n",
    "\n",
    "    ANN.train(x, y, 1, 10000)\n",
    "    \n",
    "    # Printing the new synaptic weights after training\n",
    "    #print(\"New synaptic weights after training: \")\n",
    "    #print(\"Layer 1  w_ij:\\n\", ANN.w_ij)\n",
    "    #print(\"Layer 2  w_jk:\\n\", ANN.w_jk)\n",
    "    \n",
    "    # Our prediction after feeding the ANN with new set of data\n",
    "    print(\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "    print(ANN.predict(array([[1, 0]])))\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
