{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 多分类问题\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 二分类问题\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 均方误差回归问题\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')\n",
    "\n",
    "# 自定义评估标准函数\n",
    "import keras.backend as K\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', mean_pred])\n",
    "\n",
    "\n",
    "# 对于具有 2 个类的单输入模型（二进制分类）：\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 生成虚拟数据\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# 训练模型，以 32 个样本为一个 batch 进行迭代\n",
    "model.fit(data, labels, epochs=10, batch_size=32)\n",
    "\n",
    "# 对于具有 10 个类的单输入模型（多分类分类）：\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 生成虚拟数据\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "# 将标签转换为分类的 one-hot 编码\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "# 训练模型，以 32 个样本为一个 batch 进行迭代\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16进制加法学习\n",
    "#基于多层感知器 (MLP) 的 softmax 多分类\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# 生成虚拟数据\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) 是一个具有 64 个隐藏神经元的全连接层。\n",
    "# 在第一层必须指定所期望的输入数据尺寸：\n",
    "# 在这里，是一个 20 维的向量。\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16进制加法学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "10000/10000 [==============================] - 2s 194us/step - loss: 16.4732 - accuracy: 0.1250\n",
      "Epoch 2/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 7.4060 - accuracy: 0.2909\n",
      "Epoch 3/500\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 3.7815 - accuracy: 0.5108\n",
      "Epoch 4/500\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 2.5056 - accuracy: 0.5654\n",
      "Epoch 5/500\n",
      "10000/10000 [==============================] - 1s 103us/step - loss: 1.8452 - accuracy: 0.5824\n",
      "Epoch 6/500\n",
      "10000/10000 [==============================] - 1s 103us/step - loss: 1.4080 - accuracy: 0.6285\n",
      "Epoch 7/500\n",
      "10000/10000 [==============================] - 1s 103us/step - loss: 1.0084 - accuracy: 0.6610\n",
      "Epoch 8/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.9194 - accuracy: 0.6749\n",
      "Epoch 9/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.7826 - accuracy: 0.6964\n",
      "Epoch 10/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.6649 - accuracy: 0.7186\n",
      "Epoch 11/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.5268 - accuracy: 0.7382\n",
      "Epoch 12/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.6034 - accuracy: 0.7551\n",
      "Epoch 13/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.5780 - accuracy: 0.7808\n",
      "Epoch 14/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.5029 - accuracy: 0.7896\n",
      "Epoch 15/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.5017 - accuracy: 0.8115\n",
      "Epoch 16/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.4500 - accuracy: 0.8233\n",
      "Epoch 17/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.4386 - accuracy: 0.8177\n",
      "Epoch 18/500\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.4046 - accuracy: 0.8449\n",
      "Epoch 19/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.3956 - accuracy: 0.8337\n",
      "Epoch 20/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.3913 - accuracy: 0.8510\n",
      "Epoch 21/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.3362 - accuracy: 0.8533\n",
      "Epoch 22/500\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.3240 - accuracy: 0.8577\n",
      "Epoch 23/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.3285 - accuracy: 0.8610\n",
      "Epoch 24/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.2570 - accuracy: 0.8639\n",
      "Epoch 25/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.2859 - accuracy: 0.8728\n",
      "Epoch 26/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.3064 - accuracy: 0.8791\n",
      "Epoch 27/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.2260 - accuracy: 0.8871\n",
      "Epoch 28/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.3446 - accuracy: 0.8897\n",
      "Epoch 29/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.2082 - accuracy: 0.8865\n",
      "Epoch 30/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.2852 - accuracy: 0.9134\n",
      "Epoch 31/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.2760 - accuracy: 0.8938\n",
      "Epoch 32/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.2359 - accuracy: 0.8960\n",
      "Epoch 33/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.2213 - accuracy: 0.9047\n",
      "Epoch 34/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.2275 - accuracy: 0.9121\n",
      "Epoch 35/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.2146 - accuracy: 0.9189\n",
      "Epoch 36/500\n",
      "10000/10000 [==============================] - 1s 103us/step - loss: 0.2105 - accuracy: 0.9198\n",
      "Epoch 37/500\n",
      "10000/10000 [==============================] - 1s 106us/step - loss: 0.1991 - accuracy: 0.9125\n",
      "Epoch 38/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.2027 - accuracy: 0.9257\n",
      "Epoch 39/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.1814 - accuracy: 0.9309\n",
      "Epoch 40/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.1689 - accuracy: 0.9161\n",
      "Epoch 41/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.2118 - accuracy: 0.9397\n",
      "Epoch 42/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.1983 - accuracy: 0.9385\n",
      "Epoch 43/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.1590 - accuracy: 0.9245\n",
      "Epoch 44/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.1463 - accuracy: 0.9434\n",
      "Epoch 45/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.1564 - accuracy: 0.9386\n",
      "Epoch 46/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.1679 - accuracy: 0.9478\n",
      "Epoch 47/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.1887 - accuracy: 0.9522\n",
      "Epoch 48/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.1499 - accuracy: 0.9523\n",
      "Epoch 49/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.1744 - accuracy: 0.9427\n",
      "Epoch 50/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.1510 - accuracy: 0.9355\n",
      "Epoch 51/500\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.1453 - accuracy: 0.9561\n",
      "Epoch 52/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.1509 - accuracy: 0.9530\n",
      "Epoch 53/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.2134 - accuracy: 0.9537\n",
      "Epoch 54/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.1186 - accuracy: 0.9587\n",
      "Epoch 55/500\n",
      "10000/10000 [==============================] - 1s 104us/step - loss: 0.1099 - accuracy: 0.9535\n",
      "Epoch 56/500\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.1577 - accuracy: 0.9580\n",
      "Epoch 57/500\n",
      "10000/10000 [==============================] - 1s 103us/step - loss: 0.1595 - accuracy: 0.9561\n",
      "Epoch 58/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.1067 - accuracy: 0.9597\n",
      "Epoch 59/500\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.1635 - accuracy: 0.96530s - loss: 0.1943 - accuracy\n",
      "Epoch 60/500\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.1226 - accuracy: 0.9536\n",
      "Epoch 61/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.1363 - accuracy: 0.9627\n",
      "Epoch 62/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.1556 - accuracy: 0.9606\n",
      "Epoch 63/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.1218 - accuracy: 0.9633\n",
      "Epoch 64/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.2011 - accuracy: 0.9678\n",
      "Epoch 65/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.1632 - accuracy: 0.9656\n",
      "Epoch 66/500\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.1000 - accuracy: 0.9697\n",
      "Epoch 67/500\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.0981 - accuracy: 0.9700\n",
      "Epoch 68/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0962 - accuracy: 0.9587\n",
      "Epoch 69/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.1110 - accuracy: 0.9722\n",
      "Epoch 70/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.1305 - accuracy: 0.9672\n",
      "Epoch 71/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0775 - accuracy: 0.9725\n",
      "Epoch 72/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.1251 - accuracy: 0.9729\n",
      "Epoch 73/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0839 - accuracy: 0.9591\n",
      "Epoch 74/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0845 - accuracy: 0.9770\n",
      "Epoch 75/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0910 - accuracy: 0.9738\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0918 - accuracy: 0.9732\n",
      "Epoch 77/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0668 - accuracy: 0.9717\n",
      "Epoch 78/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.1094 - accuracy: 0.9769\n",
      "Epoch 79/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0855 - accuracy: 0.9792\n",
      "Epoch 80/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0887 - accuracy: 0.9742\n",
      "Epoch 81/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0765 - accuracy: 0.9748\n",
      "Epoch 82/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0805 - accuracy: 0.9734\n",
      "Epoch 83/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0676 - accuracy: 0.9692\n",
      "Epoch 84/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0638 - accuracy: 0.9689\n",
      "Epoch 85/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0658 - accuracy: 0.9722\n",
      "Epoch 86/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0909 - accuracy: 0.9831\n",
      "Epoch 87/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0961 - accuracy: 0.9783\n",
      "Epoch 88/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0686 - accuracy: 0.9761\n",
      "Epoch 89/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0644 - accuracy: 0.9787\n",
      "Epoch 90/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0778 - accuracy: 0.9782\n",
      "Epoch 91/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0733 - accuracy: 0.9757\n",
      "Epoch 92/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0389 - accuracy: 0.9756\n",
      "Epoch 93/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0773 - accuracy: 0.9846\n",
      "Epoch 94/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0656 - accuracy: 0.9758\n",
      "Epoch 95/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0756 - accuracy: 0.9808\n",
      "Epoch 96/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0583 - accuracy: 0.9780\n",
      "Epoch 97/500\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0601 - accuracy: 0.9804\n",
      "Epoch 98/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0567 - accuracy: 0.9783\n",
      "Epoch 99/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0567 - accuracy: 0.9827\n",
      "Epoch 100/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0615 - accuracy: 0.9786\n",
      "Epoch 101/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0661 - accuracy: 0.9810\n",
      "Epoch 102/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0443 - accuracy: 0.9819\n",
      "Epoch 103/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0570 - accuracy: 0.9792\n",
      "Epoch 104/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0571 - accuracy: 0.9786\n",
      "Epoch 105/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0463 - accuracy: 0.9764\n",
      "Epoch 106/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0406 - accuracy: 0.9803\n",
      "Epoch 107/500\n",
      "10000/10000 [==============================] - 1s 106us/step - loss: 0.0557 - accuracy: 0.9809\n",
      "Epoch 108/500\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0612 - accuracy: 0.9797\n",
      "Epoch 109/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0485 - accuracy: 0.9843\n",
      "Epoch 110/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0479 - accuracy: 0.9818\n",
      "Epoch 111/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0521 - accuracy: 0.9818\n",
      "Epoch 112/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0493 - accuracy: 0.9822\n",
      "Epoch 113/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0590 - accuracy: 0.9742\n",
      "Epoch 114/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0462 - accuracy: 0.9824\n",
      "Epoch 115/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0688 - accuracy: 0.9829\n",
      "Epoch 116/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0418 - accuracy: 0.9815\n",
      "Epoch 117/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0551 - accuracy: 0.9742\n",
      "Epoch 118/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0462 - accuracy: 0.9838\n",
      "Epoch 119/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0444 - accuracy: 0.9782\n",
      "Epoch 120/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0375 - accuracy: 0.9853\n",
      "Epoch 121/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0351 - accuracy: 0.9781\n",
      "Epoch 122/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0512 - accuracy: 0.9814\n",
      "Epoch 123/500\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.0413 - accuracy: 0.9827\n",
      "Epoch 124/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0621 - accuracy: 0.9868\n",
      "Epoch 125/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0426 - accuracy: 0.9837\n",
      "Epoch 126/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0752 - accuracy: 0.9831\n",
      "Epoch 127/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0446 - accuracy: 0.9814\n",
      "Epoch 128/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0377 - accuracy: 0.9841\n",
      "Epoch 129/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0584 - accuracy: 0.9839\n",
      "Epoch 130/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0399 - accuracy: 0.9835\n",
      "Epoch 131/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0333 - accuracy: 0.9815\n",
      "Epoch 132/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0344 - accuracy: 0.9805\n",
      "Epoch 133/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0391 - accuracy: 0.9773\n",
      "Epoch 134/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0374 - accuracy: 0.9806\n",
      "Epoch 135/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0345 - accuracy: 0.9808\n",
      "Epoch 136/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0370 - accuracy: 0.9844\n",
      "Epoch 137/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0388 - accuracy: 0.9774\n",
      "Epoch 138/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0348 - accuracy: 0.9829\n",
      "Epoch 139/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0338 - accuracy: 0.9822\n",
      "Epoch 140/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0323 - accuracy: 0.9839\n",
      "Epoch 141/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0428 - accuracy: 0.9852\n",
      "Epoch 142/500\n",
      "10000/10000 [==============================] - 1s 107us/step - loss: 0.0643 - accuracy: 0.9887\n",
      "Epoch 143/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0367 - accuracy: 0.9857\n",
      "Epoch 144/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0361 - accuracy: 0.9864\n",
      "Epoch 145/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0337 - accuracy: 0.9806\n",
      "Epoch 146/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0295 - accuracy: 0.9864\n",
      "Epoch 147/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0323 - accuracy: 0.9817\n",
      "Epoch 148/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0362 - accuracy: 0.9806\n",
      "Epoch 149/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0282 - accuracy: 0.9850\n",
      "Epoch 150/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0326 - accuracy: 0.9860\n",
      "Epoch 151/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0312 - accuracy: 0.9870\n",
      "Epoch 152/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0417 - accuracy: 0.9843\n",
      "Epoch 153/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0450 - accuracy: 0.9872\n",
      "Epoch 154/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0305 - accuracy: 0.9862\n",
      "Epoch 155/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0294 - accuracy: 0.9850\n",
      "Epoch 156/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0298 - accuracy: 0.9844\n",
      "Epoch 157/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0358 - accuracy: 0.9849\n",
      "Epoch 158/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0250 - accuracy: 0.9930\n",
      "Epoch 159/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0296 - accuracy: 0.9833\n",
      "Epoch 160/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0263 - accuracy: 0.9888\n",
      "Epoch 161/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0283 - accuracy: 0.9850\n",
      "Epoch 162/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0320 - accuracy: 0.9882\n",
      "Epoch 163/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0285 - accuracy: 0.9901\n",
      "Epoch 164/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0279 - accuracy: 0.9876\n",
      "Epoch 165/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0268 - accuracy: 0.9867\n",
      "Epoch 166/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0278 - accuracy: 0.9889\n",
      "Epoch 167/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0268 - accuracy: 0.9875\n",
      "Epoch 168/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0301 - accuracy: 0.9866\n",
      "Epoch 169/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0258 - accuracy: 0.9899\n",
      "Epoch 170/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0280 - accuracy: 0.9881\n",
      "Epoch 171/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0274 - accuracy: 0.9865\n",
      "Epoch 172/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0269 - accuracy: 0.9903\n",
      "Epoch 173/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0264 - accuracy: 0.9925\n",
      "Epoch 174/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0289 - accuracy: 0.9905\n",
      "Epoch 175/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0252 - accuracy: 0.9872\n",
      "Epoch 176/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0279 - accuracy: 0.9912\n",
      "Epoch 177/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0258 - accuracy: 0.9888\n",
      "Epoch 178/500\n",
      "10000/10000 [==============================] - 1s 105us/step - loss: 0.0259 - accuracy: 0.9913\n",
      "Epoch 179/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0256 - accuracy: 0.9884\n",
      "Epoch 180/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0250 - accuracy: 0.9890\n",
      "Epoch 181/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0253 - accuracy: 0.9910\n",
      "Epoch 182/500\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0272 - accuracy: 0.9898\n",
      "Epoch 183/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0247 - accuracy: 0.9931\n",
      "Epoch 184/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0308 - accuracy: 0.9883\n",
      "Epoch 185/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0291 - accuracy: 0.9886\n",
      "Epoch 186/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0246 - accuracy: 0.9890\n",
      "Epoch 187/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0263 - accuracy: 0.9918\n",
      "Epoch 188/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0256 - accuracy: 0.9913\n",
      "Epoch 189/500\n",
      "10000/10000 [==============================] - 1s 104us/step - loss: 0.0246 - accuracy: 0.9916\n",
      "Epoch 190/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0251 - accuracy: 0.9929\n",
      "Epoch 191/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0233 - accuracy: 0.9899\n",
      "Epoch 192/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0350 - accuracy: 0.9936\n",
      "Epoch 193/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0309 - accuracy: 0.9932\n",
      "Epoch 194/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0236 - accuracy: 0.9936\n",
      "Epoch 195/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0243 - accuracy: 0.9923\n",
      "Epoch 196/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0237 - accuracy: 0.9935\n",
      "Epoch 197/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0305 - accuracy: 0.9950\n",
      "Epoch 198/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0324 - accuracy: 0.9949\n",
      "Epoch 199/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0263 - accuracy: 0.9940\n",
      "Epoch 200/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0231 - accuracy: 0.9933\n",
      "Epoch 201/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0243 - accuracy: 0.9938\n",
      "Epoch 202/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0235 - accuracy: 0.9931\n",
      "Epoch 203/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0227 - accuracy: 0.9918\n",
      "Epoch 204/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0230 - accuracy: 0.9939\n",
      "Epoch 205/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0220 - accuracy: 0.9921\n",
      "Epoch 206/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0224 - accuracy: 0.9940\n",
      "Epoch 207/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0275 - accuracy: 0.9940\n",
      "Epoch 208/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0211 - accuracy: 0.9914\n",
      "Epoch 209/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0221 - accuracy: 0.9954\n",
      "Epoch 210/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0219 - accuracy: 0.9942\n",
      "Epoch 211/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0227 - accuracy: 0.9932\n",
      "Epoch 212/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0389 - accuracy: 0.9923\n",
      "Epoch 213/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0204 - accuracy: 0.9942\n",
      "Epoch 214/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0218 - accuracy: 0.9930\n",
      "Epoch 215/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0212 - accuracy: 0.9939\n",
      "Epoch 216/500\n",
      "10000/10000 [==============================] - 1s 113us/step - loss: 0.0217 - accuracy: 0.9948\n",
      "Epoch 217/500\n",
      "10000/10000 [==============================] - 1s 107us/step - loss: 0.0219 - accuracy: 0.9919\n",
      "Epoch 218/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0205 - accuracy: 0.9919\n",
      "Epoch 219/500\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0224 - accuracy: 0.9904\n",
      "Epoch 220/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0200 - accuracy: 0.9961\n",
      "Epoch 221/500\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0223 - accuracy: 0.9938\n",
      "Epoch 222/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0516 - accuracy: 0.9919\n",
      "Epoch 223/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0216 - accuracy: 0.9937\n",
      "Epoch 224/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0211 - accuracy: 0.9937\n",
      "Epoch 225/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0203 - accuracy: 0.9915\n",
      "Epoch 226/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0211 - accuracy: 0.9922\n",
      "Epoch 227/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0238 - accuracy: 0.9949\n",
      "Epoch 228/500\n",
      "10000/10000 [==============================] - 1s 92us/step - loss: 0.0208 - accuracy: 0.9963\n",
      "Epoch 229/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0199 - accuracy: 0.9938\n",
      "Epoch 230/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0207 - accuracy: 0.9927\n",
      "Epoch 231/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0218 - accuracy: 0.9964\n",
      "Epoch 232/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0279 - accuracy: 0.9950\n",
      "Epoch 233/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0203 - accuracy: 0.9970\n",
      "Epoch 234/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0210 - accuracy: 0.9927\n",
      "Epoch 235/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0204 - accuracy: 0.9948\n",
      "Epoch 236/500\n",
      "10000/10000 [==============================] - 1s 92us/step - loss: 0.0200 - accuracy: 0.9950\n",
      "Epoch 237/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0212 - accuracy: 0.9959\n",
      "Epoch 238/500\n",
      "10000/10000 [==============================] - 1s 92us/step - loss: 0.0206 - accuracy: 0.9946\n",
      "Epoch 239/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0197 - accuracy: 0.9960\n",
      "Epoch 240/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0285 - accuracy: 0.9933\n",
      "Epoch 241/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0196 - accuracy: 0.9948\n",
      "Epoch 242/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0222 - accuracy: 0.9941\n",
      "Epoch 243/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0195 - accuracy: 0.9944\n",
      "Epoch 244/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0202 - accuracy: 0.9960\n",
      "Epoch 245/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0203 - accuracy: 0.9942\n",
      "Epoch 246/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0193 - accuracy: 0.9954\n",
      "Epoch 247/500\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0195 - accuracy: 0.9942\n",
      "Epoch 248/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0191 - accuracy: 0.9962\n",
      "Epoch 249/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0195 - accuracy: 0.9926\n",
      "Epoch 250/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0190 - accuracy: 0.9941\n",
      "Epoch 251/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0194 - accuracy: 0.9972\n",
      "Epoch 252/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0194 - accuracy: 0.9960\n",
      "Epoch 253/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0196 - accuracy: 0.9970\n",
      "Epoch 254/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0187 - accuracy: 0.9959\n",
      "Epoch 255/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0184 - accuracy: 0.9963\n",
      "Epoch 256/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0189 - accuracy: 0.9952\n",
      "Epoch 257/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0215 - accuracy: 0.9932\n",
      "Epoch 258/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0192 - accuracy: 0.9954\n",
      "Epoch 259/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0193 - accuracy: 0.9943\n",
      "Epoch 260/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0189 - accuracy: 0.9966\n",
      "Epoch 261/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0189 - accuracy: 0.9928\n",
      "Epoch 262/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0183 - accuracy: 0.9937\n",
      "Epoch 263/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0181 - accuracy: 0.9959\n",
      "Epoch 264/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0199 - accuracy: 0.9960 0s - loss: 0.0199 - accuracy\n",
      "Epoch 265/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0182 - accuracy: 0.9967\n",
      "Epoch 266/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0184 - accuracy: 0.9978\n",
      "Epoch 267/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0183 - accuracy: 0.9970\n",
      "Epoch 268/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0187 - accuracy: 0.9953\n",
      "Epoch 269/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0180 - accuracy: 0.9955\n",
      "Epoch 270/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0180 - accuracy: 0.9955\n",
      "Epoch 271/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0180 - accuracy: 0.9958\n",
      "Epoch 272/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0180 - accuracy: 0.9968\n",
      "Epoch 273/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0178 - accuracy: 0.9975\n",
      "Epoch 274/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0178 - accuracy: 0.9964\n",
      "Epoch 275/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0173 - accuracy: 0.9959\n",
      "Epoch 276/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0177 - accuracy: 0.9936\n",
      "Epoch 277/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0175 - accuracy: 0.9972\n",
      "Epoch 278/500\n",
      "10000/10000 [==============================] - 1s 104us/step - loss: 0.0173 - accuracy: 0.9958\n",
      "Epoch 279/500\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.0182 - accuracy: 0.9976\n",
      "Epoch 280/500\n",
      "10000/10000 [==============================] - 1s 112us/step - loss: 0.0173 - accuracy: 0.9961\n",
      "Epoch 281/500\n",
      "10000/10000 [==============================] - 1s 105us/step - loss: 0.0174 - accuracy: 0.9980\n",
      "Epoch 282/500\n",
      "10000/10000 [==============================] - 1s 107us/step - loss: 0.0169 - accuracy: 0.9959\n",
      "Epoch 283/500\n",
      "10000/10000 [==============================] - 1s 108us/step - loss: 0.0170 - accuracy: 0.9968\n",
      "Epoch 284/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0177 - accuracy: 0.9954\n",
      "Epoch 285/500\n",
      "10000/10000 [==============================] - 1s 103us/step - loss: 0.0203 - accuracy: 0.9967\n",
      "Epoch 286/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0166 - accuracy: 0.9979\n",
      "Epoch 287/500\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.0167 - accuracy: 0.9952\n",
      "Epoch 288/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0167 - accuracy: 0.9969\n",
      "Epoch 289/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0164 - accuracy: 0.9966\n",
      "Epoch 290/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0198 - accuracy: 0.9958\n",
      "Epoch 291/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0163 - accuracy: 0.9984\n",
      "Epoch 292/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0173 - accuracy: 0.9957\n",
      "Epoch 293/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0168 - accuracy: 0.9960\n",
      "Epoch 294/500\n",
      "10000/10000 [==============================] - 1s 110us/step - loss: 0.0165 - accuracy: 0.9972\n",
      "Epoch 295/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0162 - accuracy: 0.9971\n",
      "Epoch 296/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0168 - accuracy: 0.9991\n",
      "Epoch 297/500\n",
      "10000/10000 [==============================] - 1s 103us/step - loss: 0.0171 - accuracy: 0.9946\n",
      "Epoch 298/500\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.0153 - accuracy: 0.9963\n",
      "Epoch 299/500\n",
      "10000/10000 [==============================] - 1s 107us/step - loss: 0.0159 - accuracy: 0.9962\n",
      "Epoch 300/500\n",
      "10000/10000 [==============================] - 1s 103us/step - loss: 0.0173 - accuracy: 0.9957\n",
      "Epoch 301/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0203 - accuracy: 0.9989\n",
      "Epoch 302/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0164 - accuracy: 0.9960\n",
      "Epoch 303/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0173 - accuracy: 0.9956\n",
      "Epoch 304/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0149 - accuracy: 0.9971\n",
      "Epoch 305/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0167 - accuracy: 0.9953\n",
      "Epoch 306/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0158 - accuracy: 0.9953\n",
      "Epoch 307/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0166 - accuracy: 0.9960\n",
      "Epoch 308/500\n",
      "10000/10000 [==============================] - 1s 92us/step - loss: 0.0157 - accuracy: 0.9957\n",
      "Epoch 309/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0168 - accuracy: 0.9954\n",
      "Epoch 310/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0152 - accuracy: 0.9961\n",
      "Epoch 311/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0167 - accuracy: 0.9956\n",
      "Epoch 312/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0169 - accuracy: 0.9936\n",
      "Epoch 313/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0161 - accuracy: 0.9958\n",
      "Epoch 314/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0164 - accuracy: 0.9957\n",
      "Epoch 315/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0167 - accuracy: 0.9982\n",
      "Epoch 316/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0154 - accuracy: 0.9984\n",
      "Epoch 317/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0180 - accuracy: 0.9967\n",
      "Epoch 318/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0157 - accuracy: 0.9964\n",
      "Epoch 319/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0155 - accuracy: 0.9958\n",
      "Epoch 320/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0154 - accuracy: 0.9964\n",
      "Epoch 321/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0155 - accuracy: 0.9972\n",
      "Epoch 322/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0190 - accuracy: 0.9968\n",
      "Epoch 323/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0138 - accuracy: 0.9967\n",
      "Epoch 324/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0151 - accuracy: 0.9951\n",
      "Epoch 325/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0156 - accuracy: 0.9971\n",
      "Epoch 326/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0160 - accuracy: 0.9959\n",
      "Epoch 327/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0156 - accuracy: 0.9976\n",
      "Epoch 328/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0149 - accuracy: 0.9960\n",
      "Epoch 329/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0146 - accuracy: 0.9976\n",
      "Epoch 330/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0151 - accuracy: 0.9957\n",
      "Epoch 331/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0149 - accuracy: 0.9973\n",
      "Epoch 332/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0206 - accuracy: 0.9948\n",
      "Epoch 333/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0151 - accuracy: 0.9977\n",
      "Epoch 334/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0168 - accuracy: 0.9947\n",
      "Epoch 335/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0143 - accuracy: 0.9969\n",
      "Epoch 336/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0157 - accuracy: 0.9968\n",
      "Epoch 337/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0159 - accuracy: 0.9953\n",
      "Epoch 338/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0150 - accuracy: 0.9985\n",
      "Epoch 339/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0148 - accuracy: 0.9958\n",
      "Epoch 340/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0165 - accuracy: 0.9958\n",
      "Epoch 341/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0151 - accuracy: 0.9989\n",
      "Epoch 342/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0146 - accuracy: 0.9973\n",
      "Epoch 343/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0163 - accuracy: 0.9951\n",
      "Epoch 344/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0146 - accuracy: 0.9967\n",
      "Epoch 345/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0169 - accuracy: 0.9974\n",
      "Epoch 346/500\n",
      "10000/10000 [==============================] - 1s 102us/step - loss: 0.0151 - accuracy: 0.9958\n",
      "Epoch 347/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0161 - accuracy: 0.9952\n",
      "Epoch 348/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0152 - accuracy: 0.9970\n",
      "Epoch 349/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0151 - accuracy: 0.9955\n",
      "Epoch 350/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0153 - accuracy: 0.9965\n",
      "Epoch 351/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0148 - accuracy: 0.9962\n",
      "Epoch 352/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0152 - accuracy: 0.9966\n",
      "Epoch 353/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0151 - accuracy: 0.9967\n",
      "Epoch 354/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0148 - accuracy: 0.9979\n",
      "Epoch 355/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0148 - accuracy: 0.9973\n",
      "Epoch 356/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0151 - accuracy: 0.9973\n",
      "Epoch 357/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0156 - accuracy: 0.9980\n",
      "Epoch 358/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0159 - accuracy: 0.9967\n",
      "Epoch 359/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0148 - accuracy: 0.9949\n",
      "Epoch 360/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0191 - accuracy: 0.9982\n",
      "Epoch 361/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0152 - accuracy: 0.9966\n",
      "Epoch 362/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0156 - accuracy: 0.9971\n",
      "Epoch 363/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0156 - accuracy: 0.9971\n",
      "Epoch 364/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0202 - accuracy: 0.9973\n",
      "Epoch 365/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0141 - accuracy: 0.9971\n",
      "Epoch 366/500\n",
      "10000/10000 [==============================] - 1s 101us/step - loss: 0.0187 - accuracy: 0.9959\n",
      "Epoch 367/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0149 - accuracy: 0.9982\n",
      "Epoch 368/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0158 - accuracy: 0.9968\n",
      "Epoch 369/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0180 - accuracy: 0.9966\n",
      "Epoch 370/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0157 - accuracy: 0.9961\n",
      "Epoch 371/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0151 - accuracy: 0.9962\n",
      "Epoch 372/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0162 - accuracy: 0.9964\n",
      "Epoch 373/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0152 - accuracy: 0.9972\n",
      "Epoch 374/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0288 - accuracy: 0.9978\n",
      "Epoch 375/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0149 - accuracy: 0.9966\n",
      "Epoch 376/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0156 - accuracy: 0.9973\n",
      "Epoch 377/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0144 - accuracy: 0.9986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 378/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0169 - accuracy: 0.9963\n",
      "Epoch 379/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0161 - accuracy: 0.9965\n",
      "Epoch 380/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0149 - accuracy: 0.9967\n",
      "Epoch 381/500\n",
      "10000/10000 [==============================] - 1s 92us/step - loss: 0.0145 - accuracy: 0.9986\n",
      "Epoch 382/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0181 - accuracy: 0.9976\n",
      "Epoch 383/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0162 - accuracy: 0.9952\n",
      "Epoch 384/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0152 - accuracy: 0.9979\n",
      "Epoch 385/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0160 - accuracy: 0.9962\n",
      "Epoch 386/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0162 - accuracy: 0.9967\n",
      "Epoch 387/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0145 - accuracy: 0.9968\n",
      "Epoch 388/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0157 - accuracy: 0.9973\n",
      "Epoch 389/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0156 - accuracy: 0.9968\n",
      "Epoch 390/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0185 - accuracy: 0.9946\n",
      "Epoch 391/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0148 - accuracy: 0.9967\n",
      "Epoch 392/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0152 - accuracy: 0.9951\n",
      "Epoch 393/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0151 - accuracy: 0.9979\n",
      "Epoch 394/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0151 - accuracy: 0.9981\n",
      "Epoch 395/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0149 - accuracy: 0.9974\n",
      "Epoch 396/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0148 - accuracy: 0.9981\n",
      "Epoch 397/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0152 - accuracy: 0.9972\n",
      "Epoch 398/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0141 - accuracy: 0.9977\n",
      "Epoch 399/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0168 - accuracy: 0.9975\n",
      "Epoch 400/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0164 - accuracy: 0.9965\n",
      "Epoch 401/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0154 - accuracy: 0.9972\n",
      "Epoch 402/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0148 - accuracy: 0.9981\n",
      "Epoch 403/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0142 - accuracy: 0.9981\n",
      "Epoch 404/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0200 - accuracy: 0.9976\n",
      "Epoch 405/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0169 - accuracy: 0.9975\n",
      "Epoch 406/500\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0135 - accuracy: 0.9976\n",
      "Epoch 407/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0158 - accuracy: 0.9968\n",
      "Epoch 408/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0163 - accuracy: 0.9972\n",
      "Epoch 409/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0148 - accuracy: 0.9975\n",
      "Epoch 410/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0290 - accuracy: 0.9984\n",
      "Epoch 411/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0163 - accuracy: 0.9971\n",
      "Epoch 412/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0190 - accuracy: 0.9984\n",
      "Epoch 413/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0153 - accuracy: 0.9972\n",
      "Epoch 414/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0159 - accuracy: 0.9972\n",
      "Epoch 415/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0205 - accuracy: 0.9974\n",
      "Epoch 416/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0171 - accuracy: 0.9973\n",
      "Epoch 417/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0201 - accuracy: 0.9977\n",
      "Epoch 418/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0169 - accuracy: 0.9972\n",
      "Epoch 419/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0218 - accuracy: 0.9965\n",
      "Epoch 420/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0232 - accuracy: 0.9965\n",
      "Epoch 421/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0180 - accuracy: 0.9978\n",
      "Epoch 422/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0149 - accuracy: 0.9978\n",
      "Epoch 423/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0187 - accuracy: 0.9966\n",
      "Epoch 424/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0273 - accuracy: 0.9960\n",
      "Epoch 425/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0139 - accuracy: 0.9983\n",
      "Epoch 426/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0168 - accuracy: 0.9974\n",
      "Epoch 427/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0178 - accuracy: 0.9977\n",
      "Epoch 428/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0205 - accuracy: 0.9966\n",
      "Epoch 429/500\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0196 - accuracy: 0.9972\n",
      "Epoch 430/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0150 - accuracy: 0.9982\n",
      "Epoch 431/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0164 - accuracy: 0.9983\n",
      "Epoch 432/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0246 - accuracy: 0.9974\n",
      "Epoch 433/500\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0158 - accuracy: 0.9976\n",
      "Epoch 434/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0158 - accuracy: 0.9972\n",
      "Epoch 435/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0149 - accuracy: 0.9984\n",
      "Epoch 436/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0148 - accuracy: 0.9974\n",
      "Epoch 437/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0151 - accuracy: 0.9981\n",
      "Epoch 438/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0189 - accuracy: 0.9966\n",
      "Epoch 439/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0151 - accuracy: 0.9970\n",
      "Epoch 440/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0150 - accuracy: 0.9956\n",
      "Epoch 441/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0282 - accuracy: 0.9969\n",
      "Epoch 442/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0699 - accuracy: 0.9964\n",
      "Epoch 443/500\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0143 - accuracy: 0.9988\n",
      "Epoch 444/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0181 - accuracy: 0.9962\n",
      "Epoch 445/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0248 - accuracy: 0.9971\n",
      "Epoch 446/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0151 - accuracy: 0.9967\n",
      "Epoch 447/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0153 - accuracy: 0.9967\n",
      "Epoch 448/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0517 - accuracy: 0.9974\n",
      "Epoch 449/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0168 - accuracy: 0.9978\n",
      "Epoch 450/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0165 - accuracy: 0.9969\n",
      "Epoch 451/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0157 - accuracy: 0.9974\n",
      "Epoch 452/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0153 - accuracy: 0.9979\n",
      "Epoch 453/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0208 - accuracy: 0.9981\n",
      "Epoch 454/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0142 - accuracy: 0.9979\n",
      "Epoch 455/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0172 - accuracy: 0.9971\n",
      "Epoch 456/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0147 - accuracy: 0.9973\n",
      "Epoch 457/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0149 - accuracy: 0.9992\n",
      "Epoch 458/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0154 - accuracy: 0.9966\n",
      "Epoch 459/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0169 - accuracy: 0.9982\n",
      "Epoch 460/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0148 - accuracy: 0.9986\n",
      "Epoch 461/500\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0169 - accuracy: 0.9979\n",
      "Epoch 462/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0154 - accuracy: 0.9978\n",
      "Epoch 463/500\n",
      "10000/10000 [==============================] - 1s 92us/step - loss: 0.0152 - accuracy: 0.9972\n",
      "Epoch 464/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0145 - accuracy: 0.9979\n",
      "Epoch 465/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0145 - accuracy: 0.9983\n",
      "Epoch 466/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0147 - accuracy: 0.9982\n",
      "Epoch 467/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0148 - accuracy: 0.9985\n",
      "Epoch 468/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0146 - accuracy: 0.9986\n",
      "Epoch 469/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0145 - accuracy: 0.9987\n",
      "Epoch 470/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0143 - accuracy: 0.9976\n",
      "Epoch 471/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0141 - accuracy: 0.9990\n",
      "Epoch 472/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0188 - accuracy: 0.9982\n",
      "Epoch 473/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0167 - accuracy: 0.9973\n",
      "Epoch 474/500\n",
      "10000/10000 [==============================] - 1s 92us/step - loss: 0.0154 - accuracy: 0.9984\n",
      "Epoch 475/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0140 - accuracy: 0.9985\n",
      "Epoch 476/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0147 - accuracy: 0.9989\n",
      "Epoch 477/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0135 - accuracy: 0.9972\n",
      "Epoch 478/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0165 - accuracy: 0.9979\n",
      "Epoch 479/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0141 - accuracy: 0.9980\n",
      "Epoch 480/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0148 - accuracy: 0.9981\n",
      "Epoch 481/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0212 - accuracy: 0.9984\n",
      "Epoch 482/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0137 - accuracy: 0.9984\n",
      "Epoch 483/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0137 - accuracy: 0.9987\n",
      "Epoch 484/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0149 - accuracy: 0.9973\n",
      "Epoch 485/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0128 - accuracy: 0.9990\n",
      "Epoch 486/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0136 - accuracy: 0.9984\n",
      "Epoch 487/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0135 - accuracy: 0.9983\n",
      "Epoch 488/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0134 - accuracy: 0.9992\n",
      "Epoch 489/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0140 - accuracy: 0.9976\n",
      "Epoch 490/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0132 - accuracy: 0.9987\n",
      "Epoch 491/500\n",
      "10000/10000 [==============================] - 1s 98us/step - loss: 0.0131 - accuracy: 0.9993\n",
      "Epoch 492/500\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 0.0129 - accuracy: 0.9987\n",
      "Epoch 493/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0153 - accuracy: 0.9983\n",
      "Epoch 494/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0130 - accuracy: 0.9984\n",
      "Epoch 495/500\n",
      "10000/10000 [==============================] - 1s 93us/step - loss: 0.0136 - accuracy: 0.9994\n",
      "Epoch 496/500\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 0.0133 - accuracy: 0.9988\n",
      "Epoch 497/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0135 - accuracy: 0.9977\n",
      "Epoch 498/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0129 - accuracy: 0.9988\n",
      "Epoch 499/500\n",
      "10000/10000 [==============================] - 1s 94us/step - loss: 0.0126 - accuracy: 0.9981\n",
      "Epoch 500/500\n",
      "10000/10000 [==============================] - 1s 95us/step - loss: 0.0129 - accuracy: 0.9987\n",
      "10/10 [==============================] - 0s 16ms/step\n",
      "[0.028053879737854004, 1.0]\n",
      "[[ 4.97880793  5.        ]\n",
      " [14.84746933 15.        ]\n",
      " [11.69473076 12.        ]\n",
      " [ 0.96201491  1.        ]\n",
      " [12.740098   13.        ]\n",
      " [ 3.98865199  4.        ]\n",
      " [ 8.91725636  9.        ]\n",
      " [ 0.06512847  0.        ]\n",
      " [ 9.85578823 10.        ]\n",
      " [12.74974251 13.        ]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#基于多层感知器的二分类：\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "# LOAD DATA\n",
    "train_data = np.load('add_16.npz')\n",
    "x_train = train_data['x']\n",
    "y_train = train_data['y']\n",
    "\n",
    "test_data = np.load('add_16_t.npz')\n",
    "x_test = test_data['x']\n",
    "y_test = test_data['y']\n",
    "\n",
    "# Add the GELU function to Keras\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "get_custom_objects().update({'gelu': Activation(gelu)})\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=2, activation='gelu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='elu'))\n",
    "model.add(Dense(64, activation='selu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='selu'))\n",
    "#['sigmoid', 'relu', 'elu', 'selu', 'gelu', 'leaky-relu']\n",
    "\n",
    "model.compile(loss='mean_squared_error',#binary_crossentropy',\n",
    "              optimizer='RMSprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=500,\n",
    "          batch_size=32\n",
    "         )\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print(score)\n",
    "predict_test = model.predict(x_test)\n",
    "print(np.hstack((predict_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential， load_model\n",
    "#from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation, LeakyReLU\n",
    "#from keras.layers.noise import AlphaDropout\n",
    "#from keras.utils.generic_utils import get_custom_objects\n",
    "#from keras import backend as K\n",
    "#from keras.optimizers import Adam\n",
    "#from keras.models import load_model\n",
    "\n",
    "# LOAD DATA\n",
    "train_data = np.load('add_02.npz')\n",
    "x_train = train_data['x']\n",
    "y_train = train_data['y']\n",
    "\n",
    "#print(x_train)\n",
    "#print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readying neural network model\n",
    "def build_cnn(activation, dropout_rate, optimizer):\n",
    "    model = Sequential()\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, input_shape=(3,))) \n",
    "    model.add(Activation(activation))\n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', #损失函数（loss function），模型要将其最小化，可以通过字符串标识符/目标函数指定\n",
    "        optimizer=optimizer, #优化器（opyimizer）,如rmsprop、adagrad，或一个Optimizer类的对象\n",
    "        metrics=['accuracy'] #指标（metricts）列表， 对于任何分类问题，需要将其设置为metrics = [‘accuracy’]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Add the GELU function to Keras\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "get_custom_objects().update({'gelu': Activation(gelu)})\n",
    "\n",
    "#act_func = ['sigmoid', 'relu', 'elu', 'leaky-relu', 'selu', 'gelu']\n",
    "act_func = ['sigmoid']#扩展型指数线性单元激活函数（SELU）高斯误差线性单元激活函数（GELU）\n",
    "\n",
    "\n",
    "#model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "\n",
    "# identical to the previous one\n",
    "#model = load_model('my_model.h5')\n",
    "\n",
    "result = []\n",
    "\n",
    "for activation in act_func:\n",
    "    print('\\nTraining with -->{0}<-- activation function\\n'.format(activation))\n",
    "    \n",
    "    model = build_cnn(activation=activation,\n",
    "                      dropout_rate=0.2,\n",
    "                      optimizer=Adam(clipvalue=0.5))\n",
    "    '''\n",
    "    model.fit(x=None, y=None, \n",
    "        batch_size=None, \n",
    "        epochs=1, \n",
    "        verbose=1, \n",
    "        callbacks=None, \n",
    "        validation_split=0.0, validation_data=None, \n",
    "        shuffle=True, \n",
    "        class_weight=None, sample_weight=None, \n",
    "        initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1)\n",
    "    '''\n",
    "    history = model.fit(x_train, y_train, \n",
    "                        batch_size=None, # 128 is faster, but less accurate. 16/32 recommended\n",
    "                        epochs=100,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.0,\n",
    "                        validation_data=None ) \n",
    "    \n",
    "    result.append(history)\n",
    "    \n",
    "    K.clear_session()\n",
    "    del model\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "import numpy as np\n",
    "from numpy import exp, array, random, dot, ones_like, where, log10\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create our Artificial Neural Network class\n",
    "class ArtificialNeuralNetwork():\n",
    "    \n",
    "    # initializing the class\n",
    "    def __init__(self):\n",
    "        \n",
    "        # generating the same synaptic weights every time the program runs\n",
    "        random.seed(1)\n",
    "        \n",
    "        # synaptic weights (3 × 4 Matrix) of the hidden layer \n",
    "        self.w_ij = 2 * random.rand(3, 4) - 1\n",
    "        \n",
    "        # synaptic weights (4 × 1 Matrix) of the output layer\n",
    "        self.w_jk = 2 * random.rand(4, 1) - 1\n",
    "       \n",
    "        \n",
    "    def Sigmoid(self, x):\n",
    "        \n",
    "        # The Sigmoid activation function will turn every input value into probabilities between 0 and 1\n",
    "        # the probabilistic values help us assert which class x belongs to\n",
    "        \n",
    "        return 1 / (1 + exp(-x))\n",
    "        \n",
    "        #return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "    \n",
    "    def SigmoidDerivative(self, x):\n",
    "        \n",
    "        # The derivative of Sigmoid will be used to calculate the gradient during the backpropagation process\n",
    "        # and help optimize the random starting synaptic weights\n",
    "        \n",
    "        return x * (1 - x)\n",
    "        \n",
    "        #ax = (0.0356774 * tf.pow(x, 3) + 0.797885 * x)\n",
    "        #xx = (0.5 * tf.tanh(ax) + (0.0535161 * tf.pow(x, 3) + 0.398942 * x) * tf.pow(tf.sech(ax), 2) + 0.5)\n",
    "              \n",
    "        #return xx\n",
    "        \n",
    "    def crossentropyerror(self, a, y):\n",
    "        \n",
    "        # The cross entropy loss function\n",
    "        # we use it to evaluate the performance of our model\n",
    "        \n",
    "        return - sum(y * log10(a) + (1 - y) * log10(1 - a))\n",
    "    \n",
    "    def mael1(self, a, y):\n",
    "        \n",
    "        return np.sum(np.absolute(a - y))\n",
    "    \n",
    "    def train(self, x, y, learning_rate, iterations):\n",
    "        \n",
    "        # x: training set of data\n",
    "        # y: the actual output of the training data\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            z_ij = dot(x, self.w_ij) # the dot product of the weights of the hidden layer and the inputs\n",
    "            a_ij = self.Sigmoid(z_ij) # applying the Sigmoid activation function\n",
    "            \n",
    "            z_jk = dot(a_ij, self.w_jk) # the same previous process will be applied to find the predicted output\n",
    "            a_jk = self.Sigmoid(z_jk)  \n",
    "            \n",
    "            dl_jk = -y/a_jk + (1 - y)/(1 - a_jk) # the derivative of the cross entropy loss wrt output\n",
    "            da_jk = self.SigmoidDerivative(a_jk) # the derivative of Sigmoid  wrt the input (before activ.) of the output layer\n",
    "            dz_jk = a_ij # the derivative of the inputs of the hidden layer (before activation) wrt weights of the output layer\n",
    "            \n",
    "            dl_ij = dot(da_jk * dl_jk, self.w_jk.T) # the derivative of cross entropy loss wrt hidden layer input (after activ.)\n",
    "            da_ij = self.SigmoidDerivative(a_ij) # the derivative of Sigmoid wrt the inputs of the hidden layer (before activ.)\n",
    "            dz_ij = x # the derivative of the inputs of the hidden layer (before activation) wrt weights of the hidden layer\n",
    "            \n",
    "            # calculating the gradient using the chain rule\n",
    "            gradient_ij = dot(dz_ij.T , dl_ij * da_ij)\n",
    "            gradient_jk = dot(dz_jk.T , dl_jk * da_jk)\n",
    "            \n",
    "            # calculating the new optimal weights\n",
    "            self.w_ij = self.w_ij - learning_rate * gradient_ij \n",
    "            self.w_jk = self.w_jk - learning_rate * gradient_jk\n",
    "            \n",
    "            # printing the loss of our neural network after each 1000 iteration\n",
    "            if i % 1000 == 0 in range(iterations):\n",
    "                #print(\"loss: \", self.crossentropyerror(a_jk, y))\n",
    "                print(\"loss: \", self.mael1(a_jk, y))\n",
    "                  \n",
    "    def predict(self, inputs):\n",
    "        \n",
    "        # predicting the class of the input data after weights optimization\n",
    "        \n",
    "        output_from_layer1 = self.Sigmoid(dot(inputs, self.w_ij)) # the output of the hidden layer\n",
    "        \n",
    "        output_from_layer2 = self.Sigmoid(dot(output_from_layer1, self.w_jk)) # the output of the output layer\n",
    "        \n",
    "        return output_from_layer2\n",
    "    \n",
    "    # the function will print the initial starting weights before training\n",
    "    def SynapticWeights(self):\n",
    "        \n",
    "        print(\"Layer 1 (4 neurons, each with 3 inputs): w_ij \")        \n",
    "        print(self.w_ij)\n",
    "        \n",
    "        print(\"Layer 2 (1 neuron, with 4 inputs): w_jk \")        \n",
    "        print(self.w_jk)\n",
    "\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    ANN = ArtificialNeuralNetwork()\n",
    "    \n",
    "    ANN.SynapticWeights()\n",
    "    \n",
    "    # the training inputs \n",
    "    # the last column is used to add non linearity to the clasification task\n",
    "    x = array([[0, 0, 1], \n",
    "               [0, 1, 1], \n",
    "               [1, 0, 1], \n",
    "               [0, 1, 0], \n",
    "               [1, 0, 0], \n",
    "               [1, 1, 1], \n",
    "               [0, 0, 0]])\n",
    "    \n",
    "    # the training outputs\n",
    "    y = array([[0, 1, 1, 1, 1, 0, 0]]).T\n",
    "    #y = array([[3, 5, 5, 5, 5, 3, 3]]).T\n",
    "\n",
    "    ANN.train(x, y, 1, 10000)\n",
    "    \n",
    "    # Printing the new synaptic weights after training\n",
    "    print(\"New synaptic weights after training: \")\n",
    "    print(\"w_ij:\\n \", ANN.w_ij)\n",
    "    print(\"w_jk:\\n\", ANN.w_jk)\n",
    "    \n",
    "    # Our prediction after feeding the ANN with new set of data\n",
    "    print(\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "    print(ANN.predict(array([[1, 0, 0]])))\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
