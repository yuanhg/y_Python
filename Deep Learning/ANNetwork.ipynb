{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此练习是全过程演示神经网络如何工作，同时也可以使用如Keras等库，屏蔽神经网络内部过程\n",
    "# Import our dependencies\n",
    "import numpy as np\n",
    "from numpy import exp, array, random, dot, ones_like, where, log10\n",
    "\n",
    "# Create our Artificial Neural Network class\n",
    "class ArtificialNeuralNetwork():\n",
    "    \n",
    "    # initializing the class\n",
    "    def __init__(self):\n",
    "        \n",
    "        # generating the same synaptic weights every time the program runs\n",
    "        random.seed(1)\n",
    "        \n",
    "        # synaptic weights (3 × 4 Matrix) of the hidden layer \n",
    "        self.w_ij = 2 * random.rand(3, 4) - 1\n",
    "        \n",
    "        # synaptic weights (4 × 1 Matrix) of the output layer\n",
    "        self.w_jk = 2 * random.rand(4, 1) - 1\n",
    "        \n",
    "    def Sigmoid(self, x):\n",
    "        \n",
    "        # The Sigmoid activation function will turn every input value into \n",
    "        # probabilities between 0 and 1\n",
    "        # the probabilistic values help us assert which class x belongs to        \n",
    "        return 1 / (1 + exp(-x))\n",
    "    \n",
    "    def SigmoidDerivative(self, x):\n",
    "        \n",
    "        # The derivative of Sigmoid will be used to calculate the gradient during \n",
    "        # the backpropagation process\n",
    "        # and help optimize the random starting synaptic weights        \n",
    "        return x * (1 - x)\n",
    "        \n",
    "    def crossentropyerror(self, a, y):\n",
    "        \n",
    "        # The cross entropy loss function\n",
    "        # we use it to evaluate the performance of our model        \n",
    "        return - sum(y * log10(a) + (1 - y) * log10(1 - a))\n",
    "    \n",
    "    def train(self, x, y, learning_rate, iterations):\n",
    "        \n",
    "        # x: training set of data\n",
    "        # y: the actual output of the training data\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            \n",
    "            z_ij = dot(x, self.w_ij) # the dot product of the weights of the hidden layer and the inputs\n",
    "            a_ij = self.Sigmoid(z_ij) # applying the Sigmoid activation function\n",
    "            \n",
    "            z_jk = dot(a_ij, self.w_jk) # the same previous process will be applied to find the predicted output\n",
    "            a_jk = self.Sigmoid(z_jk)  \n",
    "            \n",
    "            dl_jk = -y/a_jk + (1 - y)/(1 - a_jk) # the derivative of the cross entropy loss wrt output\n",
    "            da_jk = self.SigmoidDerivative(a_jk) # the derivative of Sigmoid  wrt the input (before activ.) of the output layer\n",
    "            dz_jk = a_ij # the derivative of the inputs of the hidden layer (before activation) wrt weights of the output layer\n",
    "            \n",
    "            dl_ij = dot(da_jk * dl_jk, self.w_jk.T) # the derivative of cross entropy loss wrt hidden layer input (after activ.)\n",
    "            da_ij = self.SigmoidDerivative(a_ij) # the derivative of Sigmoid wrt the inputs of the hidden layer (before activ.)\n",
    "            dz_ij = x # the derivative of the inputs of the hidden layer (before activation) wrt weights of the hidden layer\n",
    "            \n",
    "            # calculating the gradient using the chain rule\n",
    "            gradient_ij = dot(dz_ij.T , dl_ij * da_ij)\n",
    "            gradient_jk = dot(dz_jk.T , dl_jk * da_jk)\n",
    "            \n",
    "            # calculating the new optimal weights\n",
    "            self.w_ij = self.w_ij - learning_rate * gradient_ij \n",
    "            self.w_jk = self.w_jk - learning_rate * gradient_jk\n",
    "            \n",
    "            # printing the loss of our neural network after each 1000 iteration\n",
    "            if i % 1000 == 0 in range(iterations):\n",
    "                print(\"loss: \", self.crossentropyerror(a_jk, y))\n",
    "                #print(\"loss: \", self.mael1(a_jk, y))\n",
    "                  \n",
    "    def predict(self, inputs):\n",
    "        \n",
    "        # predicting the class of the input data after weights optimization\n",
    "        # the output of the hidden layer\n",
    "        output_from_layer1 = self.Sigmoid(dot(inputs, self.w_ij)) \n",
    "        # the output of the output layer\n",
    "        output_from_layer2 = self.Sigmoid(dot(output_from_layer1, self.w_jk))   \n",
    "        \n",
    "        return output_from_layer2\n",
    "    \n",
    "    # the function will print the initial starting weights before training\n",
    "    def SynapticWeights(self):\n",
    "        \n",
    "        print(\"Layer 1 (4 neurons, each with 3 inputs): w_ij \")        \n",
    "        print(self.w_ij)\n",
    "        \n",
    "        print(\"Layer 2 (1 neuron, with 4 inputs): w_jk \")        \n",
    "        print(self.w_jk)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  [2.07175185]\n",
      "loss:  [0.02522354]\n",
      "loss:  [0.00812448]\n",
      "loss:  [0.00483746]\n",
      "loss:  [0.00344286]\n",
      "loss:  [0.00267204]\n",
      "loss:  [0.00218308]\n",
      "loss:  [0.00184531]\n",
      "loss:  [0.00159802]\n",
      "loss:  [0.00140915]\n",
      "Considering new situation [1, 0, 0] -> ?: \n",
      "[[0.99967377]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    ANN = ArtificialNeuralNetwork()    \n",
    "    #ANN.SynapticWeights()\n",
    "    \n",
    "    # the training inputs \n",
    "    # the last column is used to add non linearity to the clasification task\n",
    "\n",
    "    x = array([[0, 0, 1], \n",
    "               [0, 1, 1], \n",
    "               [1, 0, 1], \n",
    "               [0, 1, 0], \n",
    "               [1, 0, 0], \n",
    "               [1, 1, 1], \n",
    "               [0, 0, 0]])\n",
    "    \n",
    "    # the training outputs\n",
    "    y = array([[0, 1, 1, 1, 1, 0, 0]]).T\n",
    "\n",
    "    ANN.train(x, y, 1, 10000)\n",
    "    \n",
    "    # Printing the new synaptic weights after training\n",
    "    #print(\"New synaptic weights after training: \")\n",
    "    #print(\"Layer 1  w_ij:\\n\", ANN.w_ij)\n",
    "    #print(\"Layer 2  w_jk:\\n\", ANN.w_jk)\n",
    "    \n",
    "    # Our prediction after feeding the ANN with new set of data\n",
    "    print(\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "    print(ANN.predict(array([[1, 0, 0]])))\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此练习是全过程演示神经网络如何工作，同时也可以使用如Keras等库，屏蔽神经网络内部过程\n",
    "# Import our dependencies\n",
    "import numpy as np\n",
    "from numpy import exp, array, random, dot, ones_like, where, log10\n",
    "#import tensorflow as tf\n",
    "\n",
    "# Create our Artificial Neural Network class\n",
    "class ANN_Gelu():\n",
    "    \n",
    "    # initializing the class\n",
    "    def __init__(self):        \n",
    "        # generating the same synaptic weights every time the program runs\n",
    "        random.seed(1)        \n",
    "        # synaptic weights (2 × 64 Matrix) of the hidden layer \n",
    "        self.w_ij = 2 * random.rand(2, 4) - 1        \n",
    "        # synaptic weights (64 × 1 Matrix) of the output layer\n",
    "        self.w_jk = 2 * random.rand(4, 1) - 1\n",
    "\n",
    "    def Gelu(self, x):\n",
    "        #return 0.5*x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "        return 0.5*x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "    \n",
    "    def GeluDerivative(self, x): \n",
    "        \"\"\"\n",
    "        ax = (0.0356774 * tf.pow(x, 3) + 0.797885 * x)\n",
    "        bx = (0.5 * tf.tanh(ax) +\n",
    "              (0.0535161 * tf.pow(x, 3) + 0.398942 * x) * tf.pow(tf.sech(ax), 2) + 0.5)\n",
    "        \"\"\"\n",
    "        ax = (0.0356774 * np.power(x, 3) + 0.797885 * x)\n",
    "        #　sech / 双曲正割： sech(x) = 1 / cosh(x) \n",
    "        bx = 0.5 * np.tanh(ax)\n",
    "        cx = (0.0535161 * np.power(x, 3) + 0.398942 * x) * np.power(1/np.cosh(ax), 2)\n",
    "              \n",
    "        return bx + cx + 0.5\n",
    "        \n",
    "    def crossentropyerror(self, a, y):        \n",
    "        # The cross entropy loss function\n",
    "        # we use it to evaluate the performance of our model        \n",
    "        return - sum(y * log10(a) + (1 - y) * log10(1 - a))\n",
    "    \n",
    "    def mael1(self, a, y):        \n",
    "        return np.sum(np.absolute(a - y))\n",
    "    \n",
    "    def train(self, x, y, learning_rate=1, iterations=10000):        \n",
    "        # x: training set of data\n",
    "        # y: the actual output of the training data        \n",
    "        for i in range(iterations):            \n",
    "            z_ij = dot(x, self.w_ij) # the dot product of the weights of the hidden layer and the inputs\n",
    "            a_ij = self.Gelu(z_ij) # applying the Sigmoid activation function\n",
    "            \n",
    "            z_jk = dot(a_ij, self.w_jk) # the same previous process will be applied to find the predicted output\n",
    "            a_jk = self.Gelu(z_jk)  \n",
    "            \n",
    "            dl_jk = -y/a_jk + (1 - y)/(1 - a_jk) # the derivative of the cross entropy loss wrt output\n",
    "            da_jk = self.GeluDerivative(a_jk) # the derivative of Gelu wrt the input (before activ.) of the output layer\n",
    "            dz_jk = a_ij # the derivative of the inputs of the hidden layer (before activation) wrt weights of the output layer\n",
    "            \n",
    "            dl_ij = dot(da_jk * dl_jk, self.w_jk.T) # the derivative of cross entropy loss wrt hidden layer input (after activ.)\n",
    "            da_ij = self.GeluDerivative(a_ij) # the derivative of Sigmoid wrt the inputs of the hidden layer (before activ.)\n",
    "            dz_ij = x # the derivative of the inputs of the hidden layer (before activation) wrt weights of the hidden layer\n",
    "            \n",
    "            # calculating the gradient using the chain rule\n",
    "            gradient_ij = dot(dz_ij.T , dl_ij * da_ij)\n",
    "            gradient_jk = dot(dz_jk.T , dl_jk * da_jk)\n",
    "            \n",
    "            # calculating the new optimal weights\n",
    "            self.w_ij = self.w_ij - learning_rate * gradient_ij \n",
    "            self.w_jk = self.w_jk - learning_rate * gradient_jk\n",
    "            \n",
    "            # printing the loss of our neural network after each 1000 iteration\n",
    "            if i % 1000 == 0 in range(iterations):\n",
    "                print(\"loss: \", self.crossentropyerror(a_jk, y))\n",
    "                #print(\"loss: \", self.mael1(a_jk, y))\n",
    "                  \n",
    "    def predict(self, inputs):\n",
    "        \n",
    "        # predicting the class of the input data after weights optimization\n",
    "        # the output of the hidden layer\n",
    "        output_from_layer1 = self.Gelu(dot(inputs, self.w_ij)) \n",
    "        # the output of the output layer\n",
    "        output_from_layer2 = self.Gelu(dot(output_from_layer1, self.w_jk))   \n",
    "        \n",
    "        return output_from_layer2\n",
    "    \n",
    "    # the function will print the initial starting weights before training\n",
    "    def SynapticWeights(self):\n",
    "        \n",
    "        print(\"Layer 1 : w_ij \")        \n",
    "        print(self.w_ij)\n",
    "        \n",
    "        print(\"Layer 2 : w_jk \")        \n",
    "        print(self.w_jk)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faqui\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\faqui\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: divide by zero encountered in log10\n",
      "C:\\Users\\faqui\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in log10\n",
      "C:\\Users\\faqui\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  [nan]\n",
      "loss:  [nan]\n",
      "loss:  [nan]\n",
      "loss:  [nan]\n",
      "loss:  [nan]\n",
      "loss:  [nan]\n",
      "loss:  [nan]\n",
      "loss:  [nan]\n",
      "loss:  [nan]\n",
      "loss:  [nan]\n",
      "Considering new situation [1, 0, 0] -> ?: \n",
      "[[nan]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    ANN = ANN_Gelu()    \n",
    "    #ANN.SynapticWeights()\n",
    "    \n",
    "    # the training inputs \n",
    "    # LOAD DATA\n",
    "    train_data = np.load('add_2.npz')\n",
    "    x = train_data['x']\n",
    "    y = train_data['y']\n",
    "    \n",
    "    #print(x)\n",
    "    #print(y)\n",
    "\n",
    "    ANN.train(x, y, 1, 10000)\n",
    "    \n",
    "    # Printing the new synaptic weights after training\n",
    "    #print(\"New synaptic weights after training: \")\n",
    "    #print(\"Layer 1  w_ij:\\n\", ANN.w_ij)\n",
    "    #print(\"Layer 2  w_jk:\\n\", ANN.w_jk)\n",
    "    \n",
    "    # Our prediction after feeding the ANN with new set of data\n",
    "    print(\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "    print(ANN.predict(array([[1, 0]])))\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
